{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ccb8a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n",
      "Configuration set for processing year: 2014\n",
      "Year directory: D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\n",
      "Processed documents will be stored in: processed_output\\2014\n",
      "Chunks will be stored in: chunked_output\\2014\n",
      "Chunking parameters - Chunk size: 800, Overlap: 100\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    PyPDFLoader,\n",
    "    UnstructuredPDFLoader\n",
    ")\n",
    "\n",
    "# OCR related imports\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "\n",
    "# Cell 2: Configuration\n",
    "# Configuration parameters\n",
    "BASE_DIR = r\"D:\\Technical_projects\\PSAI\\raw_data\\PSC\"\n",
    "YEAR_TO_PROCESS = \"2014\"  # Specify the year to process\n",
    "\n",
    "# Derived paths\n",
    "YEAR_DIR = os.path.join(BASE_DIR, YEAR_TO_PROCESS)\n",
    "PROCESSED_DIR = os.path.join(\"processed_output\", YEAR_TO_PROCESS)  # Where to store the processed full documents\n",
    "CHUNKS_DIR = os.path.join(\"chunked_output\", YEAR_TO_PROCESS)  # Where to store the chunked documents\n",
    "\n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 800  # Target size for chunks in characters\n",
    "CHUNK_OVERLAP = 100  # Number of characters to overlap between chunks\n",
    "\n",
    "# OCR configuration\n",
    "# For Windows, you need to specify the path to Tesseract executable\n",
    "# If using Linux, this can be commented out\n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"  # Update this path\n",
    "pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration set for processing year: {YEAR_TO_PROCESS}\")\n",
    "print(f\"Year directory: {YEAR_DIR}\")\n",
    "print(f\"Processed documents will be stored in: {PROCESSED_DIR}\")\n",
    "print(f\"Chunks will be stored in: {CHUNKS_DIR}\")\n",
    "print(f\"Chunking parameters - Chunk size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}\")\n",
    "\n",
    "# Cell 3: OCR Functions\n",
    "def ocr_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Perform OCR on a PDF file using Tesseract.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Performing OCR on {os.path.basename(pdf_path)}...\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path)\n",
    "        \n",
    "        # Process each page\n",
    "        text_content = []\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"  Processing page {i+1}/{len(images)}\")\n",
    "            \n",
    "            # Perform OCR\n",
    "            text = pytesseract.image_to_string(image, lang='eng')\n",
    "            text_content.append(text)\n",
    "        \n",
    "        # Combine all pages\n",
    "        full_text = \"\\n\\n\".join(text_content)\n",
    "        \n",
    "        print(f\"  OCR completed. Extracted {len(full_text)} characters.\")\n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error performing OCR on {pdf_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"OCR Error: {str(e)}\"\n",
    "\n",
    "def check_pdf_has_text(pdf_path):\n",
    "    \"\"\"\n",
    "    Check if a PDF has extractable text or needs OCR.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the PDF has extractable text, False if it needs OCR\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to extract text using PyPDF2 first\n",
    "        from PyPDF2 import PdfReader\n",
    "        reader = PdfReader(pdf_path)\n",
    "        \n",
    "        # Sample the first page\n",
    "        page = reader.pages[0]\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        # Check if there's meaningful text (more than just spaces or a few characters)\n",
    "        # This is a simple heuristic - might need tuning\n",
    "        if text and len(text.strip()) > 100:\n",
    "            return True\n",
    "            \n",
    "        # If the text is too short, try another page if available\n",
    "        if len(reader.pages) > 1:\n",
    "            page = reader.pages[1]\n",
    "            text = page.extract_text()\n",
    "            if text and len(text.strip()) > 100:\n",
    "                return True\n",
    "        \n",
    "        # If we got here, PDF likely needs OCR\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking PDF text extractability: {e}\")\n",
    "        # If we couldn't check, assume OCR is needed\n",
    "        return False\n",
    "\n",
    "# Cell 4: Document Processing Functions\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date and other metadata from filename.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Original filename\n",
    "        \n",
    "    Returns:\n",
    "        dict: Metadata dictionary\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"title\": \"\",\n",
    "        \"date\": \"\",\n",
    "        \"author\": \"Phyllis Schlafly\",\n",
    "        \"subjects\": [],\n",
    "        \"page_number\": 1,\n",
    "        \"source_file\": os.path.basename(filename),\n",
    "        \"doc_type\": \"Phyllis Schlafly Column\"\n",
    "    }\n",
    "    \n",
    "    # Extract date from filename\n",
    "    date_match = re.search(r'PSC_(\\d{4})_(\\d{2})_(\\d{2})', filename)\n",
    "    \n",
    "    if date_match:\n",
    "        year, month, day = date_match.groups()\n",
    "        # Convert month number to month name\n",
    "        month_names = [\n",
    "            \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "            \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n",
    "        ]\n",
    "        try:\n",
    "            month_name = month_names[int(month) - 1]\n",
    "            metadata[\"date\"] = f\"{month_name} {int(day)}, {year}\"\n",
    "        except:\n",
    "            # Fallback if date parsing fails\n",
    "            metadata[\"date\"] = f\"{year}-{month}-{day}\"\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def get_processed_files(processed_dir):\n",
    "    \"\"\"Get list of already processed files\"\"\"\n",
    "    return [os.path.basename(f) for f in glob.glob(os.path.join(processed_dir, \"*.json\"))]\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from a file based on its extension, with OCR support.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    file_ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        if file_ext == '.docx':\n",
    "            # Use Docx2txtLoader from LangChain\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            return \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "            \n",
    "        elif file_ext == '.doc':\n",
    "            # Use UnstructuredWordDocumentLoader from LangChain\n",
    "            try:\n",
    "                loader = UnstructuredWordDocumentLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                return \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "            except Exception as e:\n",
    "                print(f\"Error with UnstructuredWordDocumentLoader: {e}\")\n",
    "                # Alternative method if the first fails\n",
    "                import textract\n",
    "                return textract.process(file_path).decode('utf-8')\n",
    "                \n",
    "        elif file_ext == '.pdf':\n",
    "            # Check if PDF has extractable text or needs OCR\n",
    "            if check_pdf_has_text(file_path):\n",
    "                print(f\"PDF has extractable text: {os.path.basename(file_path)}\")\n",
    "                # Try PyPDFLoader first\n",
    "                try:\n",
    "                    loader = PyPDFLoader(file_path)\n",
    "                    documents = loader.load()\n",
    "                    content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                    # If content is too short, likely needs OCR anyway\n",
    "                    if len(content.strip()) < 200:\n",
    "                        print(f\"Extracted text too short ({len(content.strip())} chars), falling back to OCR\")\n",
    "                        return ocr_pdf(file_path)\n",
    "                    return content\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with PyPDFLoader: {e}\")\n",
    "                    # Fall back to UnstructuredPDFLoader\n",
    "                    try:\n",
    "                        loader = UnstructuredPDFLoader(file_path)\n",
    "                        documents = loader.load()\n",
    "                        content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                        if len(content.strip()) < 200:\n",
    "                            print(f\"Extracted text too short ({len(content.strip())} chars), falling back to OCR\")\n",
    "                            return ocr_pdf(file_path)\n",
    "                        return content\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Error with UnstructuredPDFLoader: {e2}\")\n",
    "                        # If both fail, use OCR\n",
    "                        return ocr_pdf(file_path)\n",
    "            else:\n",
    "                print(f\"PDF needs OCR: {os.path.basename(file_path)}\")\n",
    "                # Perform OCR\n",
    "                return ocr_pdf(file_path)\n",
    "        else:\n",
    "            return f\"Unsupported file format: {file_ext}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return f\"Error extracting text: {str(e)}\"\n",
    "\n",
    "def process_documents_from_raw():\n",
    "    \"\"\"Process raw documents from year directory and save to processed_dir\"\"\"\n",
    "    # Get all DOCX, DOC, and PDF files\n",
    "    docx_files = glob.glob(os.path.join(YEAR_DIR, \"*.docx\"))\n",
    "    doc_files = glob.glob(os.path.join(YEAR_DIR, \"*.doc\"))\n",
    "    pdf_files = glob.glob(os.path.join(YEAR_DIR, \"*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(docx_files)} DOCX files, {len(doc_files)} DOC files, and {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    # Group files by base name\n",
    "    file_groups = {}\n",
    "    for f in docx_files + doc_files + pdf_files:\n",
    "        base_name = os.path.splitext(os.path.basename(f))[0]\n",
    "        if base_name not in file_groups:\n",
    "            file_groups[base_name] = []\n",
    "        file_groups[base_name].append(f)\n",
    "    \n",
    "    print(f\"Grouped into {len(file_groups)} unique documents\")\n",
    "    \n",
    "    # Process each group, prioritizing DOCX > DOC > PDF\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    processed_files = get_processed_files(PROCESSED_DIR)\n",
    "    \n",
    "    for base_name, files in file_groups.items():\n",
    "        # Check if this file has already been processed\n",
    "        if any(base_name in pf for pf in processed_files):\n",
    "            print(f\"Skipping {base_name} - already processed\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Sort by priority (.docx first, then .doc, then .pdf)\n",
    "        extensions = [os.path.splitext(f)[1].lower() for f in files]\n",
    "        priorities = []\n",
    "        for ext in extensions:\n",
    "            if ext == \".docx\":\n",
    "                priorities.append(0)\n",
    "            elif ext == \".doc\":\n",
    "                priorities.append(1)\n",
    "            elif ext == \".pdf\":\n",
    "                priorities.append(2)\n",
    "            else:\n",
    "                priorities.append(3)\n",
    "        \n",
    "        # Get the file with highest priority (lowest number)\n",
    "        best_file = files[priorities.index(min(priorities))]\n",
    "        print(f\"Processing {os.path.basename(best_file)}\")\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = extract_metadata_from_filename(best_file)\n",
    "        \n",
    "        # Extract text\n",
    "        content = extract_text_from_file(best_file)\n",
    "        \n",
    "        # Create the document object\n",
    "        document = {\n",
    "            \"text\": content,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "        # Save the processed document\n",
    "        output_file = os.path.join(PROCESSED_DIR, f\"{base_name}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(document, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        processed_count += 1\n",
    "    \n",
    "    print(f\"Processed {processed_count} new documents, skipped {skipped_count} already processed\")\n",
    "    return processed_count\n",
    "\n",
    "# Cell 5: Document Loading Functions\n",
    "def load_processed_documents() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all processed documents from the processed directory.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of processed document dictionaries\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Get all JSON files in the processed directory\n",
    "    json_files = glob.glob(os.path.join(PROCESSED_DIR, \"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} processed documents\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                document = json.load(f)\n",
    "                documents.append(document)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading document {json_file}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Cell 6: Chunking Function with LangChain\n",
    "def chunk_document(document: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Split a document into chunks using LangChain's RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        document (Dict[str, Any]): The document to chunk\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of document chunks\n",
    "    \"\"\"\n",
    "    text = document.get(\"text\", \"\")\n",
    "    metadata = document.get(\"metadata\", {})\n",
    "    \n",
    "    # Handle empty documents\n",
    "    if not text:\n",
    "        print(f\"Warning: Empty text in document {metadata.get('source_file', 'unknown')}\")\n",
    "        return []\n",
    "    \n",
    "    # Create a text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunk_texts = text_splitter.split_text(text)\n",
    "    \n",
    "    # Create document chunks with metadata\n",
    "    chunks = []\n",
    "    for i, chunk_text in enumerate(chunk_texts):\n",
    "        # Create a copy of the metadata\n",
    "        chunk_metadata = metadata.copy()\n",
    "        \n",
    "        # Add chunk information to metadata\n",
    "        chunk_metadata[\"chunk_id\"] = i + 1\n",
    "        chunk_metadata[\"total_chunks\"] = len(chunk_texts)\n",
    "        \n",
    "        # Create chunk\n",
    "        chunk = {\n",
    "            \"text\": chunk_text,\n",
    "            \"metadata\": chunk_metadata\n",
    "        }\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Cell 7: Process and Chunk Documents\n",
    "def process_chunks_for_documents() -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Process all documents and create chunks.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[int, int]: (Number of documents processed, Number of chunks created)\n",
    "    \"\"\"\n",
    "    # Load the processed documents\n",
    "    documents = load_processed_documents()\n",
    "    \n",
    "    total_chunks = 0\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    # Check if we have any documents to process\n",
    "    if total_docs == 0:\n",
    "        print(\"No documents found to chunk. Please run the document processing first.\")\n",
    "        return 0, 0\n",
    "    \n",
    "    # Process each document\n",
    "    for doc_index, document in enumerate(documents):\n",
    "        source_file = document[\"metadata\"].get(\"source_file\", f\"document_{doc_index}\")\n",
    "        print(f\"Processing document {doc_index+1}/{total_docs}: {source_file}\")\n",
    "        \n",
    "        # Create chunks for this document\n",
    "        chunks = chunk_document(document)\n",
    "        \n",
    "        # Save each chunk as a separate JSON file\n",
    "        base_name = os.path.splitext(source_file)[0]\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_file = os.path.join(\n",
    "                CHUNKS_DIR, \n",
    "                f\"{base_name}_chunk_{i+1}of{len(chunks)}.json\"\n",
    "            )\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(chunk, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"  Created {len(chunks)} chunks\")\n",
    "        total_chunks += len(chunks)\n",
    "    \n",
    "    # Save a summary file\n",
    "    summary_data = {\n",
    "        \"year\": YEAR_TO_PROCESS,\n",
    "        \"documents_processed\": total_docs,\n",
    "        \"chunks_created\": total_chunks,\n",
    "        \"chunk_params\": {\n",
    "            \"chunk_size\": CHUNK_SIZE,\n",
    "            \"chunk_overlap\": CHUNK_OVERLAP\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = os.path.join(CHUNKS_DIR, f\"chunks_summary.json\")\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return total_docs, total_chunks\n",
    "\n",
    "# Cell 8: Chunk Analysis\n",
    "def analyze_chunks() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the created chunks and generate statistics.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: Analysis results\n",
    "    \"\"\"\n",
    "    # Get all chunk files\n",
    "    chunk_pattern = os.path.join(CHUNKS_DIR, \"*_chunk_*.json\")\n",
    "    chunk_files = glob.glob(chunk_pattern)\n",
    "    \n",
    "    # If no chunks found, return empty analysis\n",
    "    if not chunk_files:\n",
    "        print(\"No chunks found to analyze.\")\n",
    "        return {\n",
    "            \"year\": YEAR_TO_PROCESS,\n",
    "            \"total_chunks\": 0,\n",
    "            \"chunk_size_stats\": {\n",
    "                \"min\": 0, \"max\": 0, \"mean\": 0, \"median\": 0, \"std\": 0\n",
    "            },\n",
    "            \"chunks_per_doc_stats\": {\n",
    "                \"min\": 0, \"max\": 0, \"mean\": 0, \"median\": 0, \"total_docs\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Collect statistics\n",
    "    chunk_sizes = []\n",
    "    chunks_per_doc = {}\n",
    "    \n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "                chunk = json.load(f)\n",
    "                \n",
    "                # Get chunk size in characters\n",
    "                chunk_size = len(chunk[\"text\"])\n",
    "                chunk_sizes.append(chunk_size)\n",
    "                \n",
    "                # Track chunks per document\n",
    "                source_file = chunk[\"metadata\"].get(\"source_file\", \"unknown\")\n",
    "                if source_file not in chunks_per_doc:\n",
    "                    chunks_per_doc[source_file] = 0\n",
    "                chunks_per_doc[source_file] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing chunk {chunk_file}: {e}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    analysis = {\n",
    "        \"year\": YEAR_TO_PROCESS,\n",
    "        \"total_chunks\": len(chunk_sizes),\n",
    "        \"chunk_size_stats\": {\n",
    "            \"min\": min(chunk_sizes) if chunk_sizes else 0,\n",
    "            \"max\": max(chunk_sizes) if chunk_sizes else 0,\n",
    "            \"mean\": float(np.mean(chunk_sizes)) if chunk_sizes else 0,\n",
    "            \"median\": float(np.median(chunk_sizes)) if chunk_sizes else 0,\n",
    "            \"std\": float(np.std(chunk_sizes)) if chunk_sizes else 0\n",
    "        },\n",
    "        \"chunks_per_doc_stats\": {\n",
    "            \"min\": min(chunks_per_doc.values()) if chunks_per_doc else 0,\n",
    "            \"max\": max(chunks_per_doc.values()) if chunks_per_doc else 0,\n",
    "            \"mean\": float(np.mean(list(chunks_per_doc.values()))) if chunks_per_doc else 0,\n",
    "            \"median\": float(np.median(list(chunks_per_doc.values()))) if chunks_per_doc else 0,\n",
    "            \"total_docs\": len(chunks_per_doc)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save analysis\n",
    "    analysis_file = os.path.join(CHUNKS_DIR, f\"chunk_analysis.json\")\n",
    "    with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "80d8ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 DOCX files, 22 DOC files, and 1 PDF files\n",
      "Grouped into 51 unique documents\n",
      "Processing PSC_2014_01_14.docx\n",
      "Processing PSC_2014_01_28.docx\n",
      "Processing PSC_2014_02_11.docx\n",
      "Processing PSC_2014_02_18.docx\n",
      "Processing PSC_2014_03_04.docx\n",
      "Processing PSC_2014_03_11.docx\n",
      "Processing PSC_2014_03_25.docx\n",
      "Processing PSC_2014_04_01.docx\n",
      "Processing PSC_2014_04_08.docx\n",
      "Processing PSC_2014_04_15.docx\n",
      "Processing PSC_2014_04_22.docx\n",
      "Processing PSC_2014_04_29.docx\n",
      "Processing PSC_2014_05_06.docx\n",
      "Processing PSC_2014_05_13.docx\n",
      "Processing PSC_2014_05_20.docx\n",
      "Processing PSC_2014_05_27.docx\n",
      "Processing PSC_2014_06_03.docx\n",
      "Processing PSC_2014_06_10.docx\n",
      "Processing PSC_2014_06_17.docx\n",
      "Processing PSC_2014_06_24.docx\n",
      "Processing PSC_2014_07_02.docx\n",
      "Processing PSC_2014_07_09.docx\n",
      "Processing PSC_2014_07_16.docx\n",
      "Processing PSC_2014_07_22.docx\n",
      "Processing PSC_2014_08_05.docx\n",
      "Processing PSC_2014_08_12.docx\n",
      "Processing PSC_2017_07_29.docx\n",
      "Processing ~$c042914.docx\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\~$c042914.docx: File is not a zip file\n",
      "Processing PSC_2014_01_21.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_01_21.doc: No module named 'textract'\n",
      "Processing PSC_2014_03_18.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_03_18.doc: No module named 'textract'\n",
      "Processing PSC_2014_08_19.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_08_19.doc: No module named 'textract'\n",
      "Processing PSC_2014_08_26.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_08_26.doc: No module named 'textract'\n",
      "Processing PSC_2014_09_02.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_09_02.doc: No module named 'textract'\n",
      "Processing PSC_2014_09_09.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_09_09.doc: No module named 'textract'\n",
      "Processing PSC_2014_09_16.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_09_16.doc: No module named 'textract'\n",
      "Processing PSC_2014_09_24.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_09_24.doc: No module named 'textract'\n",
      "Processing PSC_2014_09_30.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_09_30.doc: No module named 'textract'\n",
      "Processing PSC_2014_10_07.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_10_07.doc: No module named 'textract'\n",
      "Processing PSC_2014_10_14.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_10_14.doc: No module named 'textract'\n",
      "Processing PSC_2014_10_21.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_10_21.doc: No module named 'textract'\n",
      "Processing PSC_2014_10_28.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_10_28.doc: No module named 'textract'\n",
      "Processing PSC_2014_11_04.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_11_04.doc: No module named 'textract'\n",
      "Processing PSC_2014_11_11.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_11_11.doc: No module named 'textract'\n",
      "Processing PSC_2014_11_18.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_11_18.doc: No module named 'textract'\n",
      "Processing PSC_2014_11_25.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_11_25.doc: No module named 'textract'\n",
      "Processing PSC_2014_12_09.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_12_09.doc: No module named 'textract'\n",
      "Processing PSC_2014_12_16.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_12_16.doc: No module named 'textract'\n",
      "Processing PSC_2014_12_23.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_12_23.doc: No module named 'textract'\n",
      "Processing PSC_2014_12_30.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\PSC_2014_12_30.doc: No module named 'textract'\n",
      "Processing ~$c082614.doc\n",
      "Error with UnstructuredWordDocumentLoader: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "Error extracting text from D:\\Technical_projects\\PSAI\\raw_data\\PSC\\2014\\~$c082614.doc: No module named 'textract'\n",
      "Processing PSC_2014_01_07.pdf\n",
      "PDF has extractable text: PSC_2014_01_07.pdf\n",
      "Processed 51 new documents, skipped 0 already processed\n",
      "Processed 51 new documents\n",
      "Found 51 processed documents\n",
      "Processing document 1/51: PSC_2014_01_07.pdf\n",
      "  Created 8 chunks\n",
      "Processing document 2/51: PSC_2014_01_14.docx\n",
      "  Created 9 chunks\n",
      "Processing document 3/51: PSC_2014_01_21.doc\n",
      "  Created 1 chunks\n",
      "Processing document 4/51: PSC_2014_01_28.docx\n",
      "  Created 9 chunks\n",
      "Processing document 5/51: PSC_2014_02_11.docx\n",
      "  Created 11 chunks\n",
      "Processing document 6/51: PSC_2014_02_18.docx\n",
      "  Created 10 chunks\n",
      "Processing document 7/51: PSC_2014_03_04.docx\n",
      "  Created 9 chunks\n",
      "Processing document 8/51: PSC_2014_03_11.docx\n",
      "  Created 9 chunks\n",
      "Processing document 9/51: PSC_2014_03_18.doc\n",
      "  Created 1 chunks\n",
      "Processing document 10/51: PSC_2014_03_25.docx\n",
      "  Created 9 chunks\n",
      "Processing document 11/51: PSC_2014_04_01.docx\n",
      "  Created 9 chunks\n",
      "Processing document 12/51: PSC_2014_04_08.docx\n",
      "  Created 8 chunks\n",
      "Processing document 13/51: PSC_2014_04_15.docx\n",
      "  Created 8 chunks\n",
      "Processing document 14/51: PSC_2014_04_22.docx\n",
      "  Created 9 chunks\n",
      "Processing document 15/51: PSC_2014_04_29.docx\n",
      "  Created 9 chunks\n",
      "Processing document 16/51: PSC_2014_05_06.docx\n",
      "  Created 9 chunks\n",
      "Processing document 17/51: PSC_2014_05_13.docx\n",
      "  Created 9 chunks\n",
      "Processing document 18/51: PSC_2014_05_20.docx\n",
      "  Created 9 chunks\n",
      "Processing document 19/51: PSC_2014_05_27.docx\n",
      "  Created 9 chunks\n",
      "Processing document 20/51: PSC_2014_06_03.docx\n",
      "  Created 8 chunks\n",
      "Processing document 21/51: PSC_2014_06_10.docx\n",
      "  Created 8 chunks\n",
      "Processing document 22/51: PSC_2014_06_17.docx\n",
      "  Created 9 chunks\n",
      "Processing document 23/51: PSC_2014_06_24.docx\n",
      "  Created 10 chunks\n",
      "Processing document 24/51: PSC_2014_07_02.docx\n",
      "  Created 10 chunks\n",
      "Processing document 25/51: PSC_2014_07_09.docx\n",
      "  Created 8 chunks\n",
      "Processing document 26/51: PSC_2014_07_16.docx\n",
      "  Created 9 chunks\n",
      "Processing document 27/51: PSC_2014_07_22.docx\n",
      "  Created 9 chunks\n",
      "Processing document 28/51: PSC_2014_08_05.docx\n",
      "  Created 9 chunks\n",
      "Processing document 29/51: PSC_2014_08_12.docx\n",
      "  Created 10 chunks\n",
      "Processing document 30/51: PSC_2014_08_19.doc\n",
      "  Created 1 chunks\n",
      "Processing document 31/51: PSC_2014_08_26.doc\n",
      "  Created 1 chunks\n",
      "Processing document 32/51: PSC_2014_09_02.doc\n",
      "  Created 1 chunks\n",
      "Processing document 33/51: PSC_2014_09_09.doc\n",
      "  Created 1 chunks\n",
      "Processing document 34/51: PSC_2014_09_16.doc\n",
      "  Created 1 chunks\n",
      "Processing document 35/51: PSC_2014_09_24.doc\n",
      "  Created 1 chunks\n",
      "Processing document 36/51: PSC_2014_09_30.doc\n",
      "  Created 1 chunks\n",
      "Processing document 37/51: PSC_2014_10_07.doc\n",
      "  Created 1 chunks\n",
      "Processing document 38/51: PSC_2014_10_14.doc\n",
      "  Created 1 chunks\n",
      "Processing document 39/51: PSC_2014_10_21.doc\n",
      "  Created 1 chunks\n",
      "Processing document 40/51: PSC_2014_10_28.doc\n",
      "  Created 1 chunks\n",
      "Processing document 41/51: PSC_2014_11_04.doc\n",
      "  Created 1 chunks\n",
      "Processing document 42/51: PSC_2014_11_11.doc\n",
      "  Created 1 chunks\n",
      "Processing document 43/51: PSC_2014_11_18.doc\n",
      "  Created 1 chunks\n",
      "Processing document 44/51: PSC_2014_11_25.doc\n",
      "  Created 1 chunks\n",
      "Processing document 45/51: PSC_2014_12_09.doc\n",
      "  Created 1 chunks\n",
      "Processing document 46/51: PSC_2014_12_16.doc\n",
      "  Created 1 chunks\n",
      "Processing document 47/51: PSC_2014_12_23.doc\n",
      "  Created 1 chunks\n",
      "Processing document 48/51: PSC_2014_12_30.doc\n",
      "  Created 1 chunks\n",
      "Processing document 49/51: PSC_2017_07_29.docx\n",
      "  Created 10 chunks\n",
      "Processing document 50/51: ~$c042914.docx\n",
      "  Created 1 chunks\n",
      "Processing document 51/51: ~$c082614.doc\n",
      "  Created 1 chunks\n",
      "\n",
      "Chunked 51 documents into 276 chunks for year 2014\n",
      "\n",
      "Chunk Analysis:\n",
      "Total chunks: 276\n",
      "Average chunk size: 545.1 characters\n",
      "Average chunks per document: 5.4\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Main Execution\n",
    "try:\n",
    "    # First, process any raw documents that need processing\n",
    "    new_docs_processed = process_documents_from_raw()\n",
    "    print(f\"Processed {new_docs_processed} new documents\")\n",
    "\n",
    "    # Now, chunk all processed documents\n",
    "    total_docs, total_chunks = process_chunks_for_documents()\n",
    "    print(f\"\\nChunked {total_docs} documents into {total_chunks} chunks for year {YEAR_TO_PROCESS}\")\n",
    "\n",
    "    # Only analyze if we have chunks\n",
    "    if total_chunks > 0:\n",
    "        # Analyze chunks\n",
    "        analysis = analyze_chunks()\n",
    "        print(\"\\nChunk Analysis:\")\n",
    "        print(f\"Total chunks: {analysis['total_chunks']}\")\n",
    "        print(f\"Average chunk size: {analysis['chunk_size_stats']['mean']:.1f} characters\")\n",
    "        print(f\"Average chunks per document: {analysis['chunks_per_doc_stats']['mean']:.1f}\")\n",
    "    else:\n",
    "        print(\"\\nNo chunks to analyze.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Make sure all required libraries are installed\")\n",
    "    print(\"2. Verify folder paths exist and are accessible\")\n",
    "    print(\"3. Check that document files are readable\")\n",
    "    print(\"4. Ensure Tesseract OCR is properly installed and the path is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bbfdc50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n",
      "Consolidated files will be saved to: consolidated_output\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "\n",
    "# Cell 2: Configuration\n",
    "# Base directories\n",
    "CHUNKS_BASE_DIR = \"chunked_output\"  # Base directory for chunked files\n",
    "CONSOLIDATED_DIR = \"consolidated_output\"  # Directory to save consolidated files\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(CONSOLIDATED_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Consolidated files will be saved to: {CONSOLIDATED_DIR}\")\n",
    "\n",
    "# Cell 3: Consolidation Function\n",
    "def consolidate_year_chunks(year, input_dir=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Consolidate all JSON chunk files for a specific year into a single JSON file.\n",
    "    \n",
    "    Args:\n",
    "        year (str): The year to process\n",
    "        input_dir (str, optional): Base directory for chunked files. Defaults to \"chunked_output\".\n",
    "        output_dir (str, optional): Directory to save consolidated files. Defaults to \"consolidated_output\".\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Number of chunks processed, Total file size)\n",
    "    \"\"\"\n",
    "    # Set up directories\n",
    "    if input_dir is None:\n",
    "        input_dir = CHUNKS_BASE_DIR\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = CONSOLIDATED_DIR\n",
    "    \n",
    "    year_input_dir = os.path.join(input_dir, year)\n",
    "    \n",
    "    # Check if the year directory exists\n",
    "    if not os.path.exists(year_input_dir):\n",
    "        print(f\"Directory not found for year {year}: {year_input_dir}\")\n",
    "        return 0, 0\n",
    "    \n",
    "    # Get all JSON chunk files for this year\n",
    "    chunk_pattern = os.path.join(year_input_dir, \"*_chunk_*.json\")\n",
    "    chunk_files = glob.glob(chunk_pattern)\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(f\"No chunk files found for year {year} in {year_input_dir}\")\n",
    "        return 0, 0\n",
    "    \n",
    "    print(f\"Found {len(chunk_files)} chunk files for year {year}\")\n",
    "    \n",
    "    # Load all chunks\n",
    "    all_chunks = []\n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "                chunk = json.load(f)\n",
    "                all_chunks.append(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {chunk_file}: {e}\")\n",
    "    \n",
    "    # Sort chunks by date (if available) and then by source_file and chunk_id\n",
    "    def chunk_sort_key(chunk):\n",
    "        metadata = chunk.get(\"metadata\", {})\n",
    "        date = metadata.get(\"date\", \"\")\n",
    "        source_file = metadata.get(\"source_file\", \"\")\n",
    "        chunk_id = metadata.get(\"chunk_id\", 0)\n",
    "        return (date, source_file, chunk_id)\n",
    "    \n",
    "    all_chunks.sort(key=chunk_sort_key)\n",
    "    \n",
    "    # Create the consolidated JSON file\n",
    "    output_file = os.path.join(output_dir, f\"psc_{year}_all_chunks.json\")\n",
    "    \n",
    "    # Create a JSON object with metadata and the chunks\n",
    "    consolidated_data = {\n",
    "        \"year\": year,\n",
    "        \"chunk_count\": len(all_chunks),\n",
    "        \"chunks\": all_chunks\n",
    "    }\n",
    "    \n",
    "    # Save the consolidated file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(consolidated_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(output_file)\n",
    "    file_size_mb = file_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Created consolidated file: {output_file}\")\n",
    "    print(f\"File contains {len(all_chunks)} chunks\")\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    return len(all_chunks), file_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Cell 4: Process a Single Year\n",
    "# # Change this to the year you want to process\n",
    "# YEAR_TO_PROCESS = \"1973\"\n",
    "\n",
    "# # Process the year\n",
    "# chunks, size = consolidate_year_chunks(YEAR_TO_PROCESS)\n",
    "\n",
    "# if chunks > 0:\n",
    "#     size_mb = size / (1024 * 1024)\n",
    "#     print(f\"\\nSuccessfully consolidated {chunks} chunks for year {YEAR_TO_PROCESS}\")\n",
    "#     print(f\"File size: {size_mb:.2f} MB\")\n",
    "# else:\n",
    "#     print(f\"\\nNo chunks were processed for year {YEAR_TO_PROCESS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "62e4aaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42 years to process: 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014\n",
      "\n",
      "Processing year 1973...\n",
      "Found 218 chunk files for year 1973\n",
      "Created consolidated file: consolidated_output\\psc_1973_all_chunks.json\n",
      "File contains 218 chunks\n",
      "File size: 0.20 MB\n",
      "\n",
      "Processing year 1974...\n",
      "Found 1043 chunk files for year 1974\n",
      "Created consolidated file: consolidated_output\\psc_1974_all_chunks.json\n",
      "File contains 1043 chunks\n",
      "File size: 0.92 MB\n",
      "\n",
      "Processing year 1975...\n",
      "Found 587 chunk files for year 1975\n",
      "Created consolidated file: consolidated_output\\psc_1975_all_chunks.json\n",
      "File contains 587 chunks\n",
      "File size: 0.53 MB\n",
      "\n",
      "Processing year 1976...\n",
      "Found 623 chunk files for year 1976\n",
      "Created consolidated file: consolidated_output\\psc_1976_all_chunks.json\n",
      "File contains 623 chunks\n",
      "File size: 0.57 MB\n",
      "\n",
      "Processing year 1977...\n",
      "Found 671 chunk files for year 1977\n",
      "Created consolidated file: consolidated_output\\psc_1977_all_chunks.json\n",
      "File contains 671 chunks\n",
      "File size: 0.62 MB\n",
      "\n",
      "Processing year 1978...\n",
      "Found 736 chunk files for year 1978\n",
      "Created consolidated file: consolidated_output\\psc_1978_all_chunks.json\n",
      "File contains 736 chunks\n",
      "File size: 0.67 MB\n",
      "\n",
      "Processing year 1979...\n",
      "Found 764 chunk files for year 1979\n",
      "Created consolidated file: consolidated_output\\psc_1979_all_chunks.json\n",
      "File contains 764 chunks\n",
      "File size: 0.72 MB\n",
      "\n",
      "Processing year 1980...\n",
      "Found 817 chunk files for year 1980\n",
      "Created consolidated file: consolidated_output\\psc_1980_all_chunks.json\n",
      "File contains 817 chunks\n",
      "File size: 0.77 MB\n",
      "\n",
      "Processing year 1981...\n",
      "Found 829 chunk files for year 1981\n",
      "Created consolidated file: consolidated_output\\psc_1981_all_chunks.json\n",
      "File contains 829 chunks\n",
      "File size: 0.79 MB\n",
      "\n",
      "Processing year 1982...\n",
      "Found 843 chunk files for year 1982\n",
      "Created consolidated file: consolidated_output\\psc_1982_all_chunks.json\n",
      "File contains 843 chunks\n",
      "File size: 0.81 MB\n",
      "\n",
      "Processing year 1983...\n",
      "Found 1392 chunk files for year 1983\n",
      "Created consolidated file: consolidated_output\\psc_1983_all_chunks.json\n",
      "File contains 1392 chunks\n",
      "File size: 0.95 MB\n",
      "\n",
      "Processing year 1984...\n",
      "Found 793 chunk files for year 1984\n",
      "Created consolidated file: consolidated_output\\psc_1984_all_chunks.json\n",
      "File contains 793 chunks\n",
      "File size: 0.74 MB\n",
      "\n",
      "Processing year 1985...\n",
      "Found 802 chunk files for year 1985\n",
      "Created consolidated file: consolidated_output\\psc_1985_all_chunks.json\n",
      "File contains 802 chunks\n",
      "File size: 0.76 MB\n",
      "\n",
      "Processing year 1986...\n",
      "Found 820 chunk files for year 1986\n",
      "Created consolidated file: consolidated_output\\psc_1986_all_chunks.json\n",
      "File contains 820 chunks\n",
      "File size: 0.77 MB\n",
      "\n",
      "Processing year 1987...\n",
      "Found 824 chunk files for year 1987\n",
      "Created consolidated file: consolidated_output\\psc_1987_all_chunks.json\n",
      "File contains 824 chunks\n",
      "File size: 0.80 MB\n",
      "\n",
      "Processing year 1988...\n",
      "Found 810 chunk files for year 1988\n",
      "Created consolidated file: consolidated_output\\psc_1988_all_chunks.json\n",
      "File contains 810 chunks\n",
      "File size: 0.77 MB\n",
      "\n",
      "Processing year 1989...\n",
      "Found 675 chunk files for year 1989\n",
      "Created consolidated file: consolidated_output\\psc_1989_all_chunks.json\n",
      "File contains 675 chunks\n",
      "File size: 0.64 MB\n",
      "\n",
      "Processing year 1990...\n",
      "Found 445 chunk files for year 1990\n",
      "Created consolidated file: consolidated_output\\psc_1990_all_chunks.json\n",
      "File contains 445 chunks\n",
      "File size: 0.45 MB\n",
      "\n",
      "Processing year 1991...\n",
      "Found 431 chunk files for year 1991\n",
      "Created consolidated file: consolidated_output\\psc_1991_all_chunks.json\n",
      "File contains 431 chunks\n",
      "File size: 0.42 MB\n",
      "\n",
      "Processing year 1992...\n",
      "Found 452 chunk files for year 1992\n",
      "Created consolidated file: consolidated_output\\psc_1992_all_chunks.json\n",
      "File contains 452 chunks\n",
      "File size: 0.43 MB\n",
      "\n",
      "Processing year 1993...\n",
      "Found 433 chunk files for year 1993\n",
      "Created consolidated file: consolidated_output\\psc_1993_all_chunks.json\n",
      "File contains 433 chunks\n",
      "File size: 0.41 MB\n",
      "\n",
      "Processing year 1994...\n",
      "Found 430 chunk files for year 1994\n",
      "Created consolidated file: consolidated_output\\psc_1994_all_chunks.json\n",
      "File contains 430 chunks\n",
      "File size: 0.41 MB\n",
      "\n",
      "Processing year 1995...\n",
      "Found 433 chunk files for year 1995\n",
      "Created consolidated file: consolidated_output\\psc_1995_all_chunks.json\n",
      "File contains 433 chunks\n",
      "File size: 0.41 MB\n",
      "\n",
      "Processing year 1996...\n",
      "Found 434 chunk files for year 1996\n",
      "Created consolidated file: consolidated_output\\psc_1996_all_chunks.json\n",
      "File contains 434 chunks\n",
      "File size: 0.41 MB\n",
      "\n",
      "Processing year 1997...\n",
      "Found 461 chunk files for year 1997\n",
      "Created consolidated file: consolidated_output\\psc_1997_all_chunks.json\n",
      "File contains 461 chunks\n",
      "File size: 0.43 MB\n",
      "\n",
      "Processing year 1998...\n",
      "Found 456 chunk files for year 1998\n",
      "Created consolidated file: consolidated_output\\psc_1998_all_chunks.json\n",
      "File contains 456 chunks\n",
      "File size: 0.42 MB\n",
      "\n",
      "Processing year 1999...\n",
      "Found 448 chunk files for year 1999\n",
      "Created consolidated file: consolidated_output\\psc_1999_all_chunks.json\n",
      "File contains 448 chunks\n",
      "File size: 0.42 MB\n",
      "\n",
      "Processing year 2000...\n",
      "Found 458 chunk files for year 2000\n",
      "Created consolidated file: consolidated_output\\psc_2000_all_chunks.json\n",
      "File contains 458 chunks\n",
      "File size: 0.42 MB\n",
      "\n",
      "Processing year 2001...\n",
      "Found 428 chunk files for year 2001\n",
      "Created consolidated file: consolidated_output\\psc_2001_all_chunks.json\n",
      "File contains 428 chunks\n",
      "File size: 0.40 MB\n",
      "\n",
      "Processing year 2002...\n",
      "Found 441 chunk files for year 2002\n",
      "Created consolidated file: consolidated_output\\psc_2002_all_chunks.json\n",
      "File contains 441 chunks\n",
      "File size: 0.41 MB\n",
      "\n",
      "Processing year 2003...\n",
      "Found 459 chunk files for year 2003\n",
      "Created consolidated file: consolidated_output\\psc_2003_all_chunks.json\n",
      "File contains 459 chunks\n",
      "File size: 0.43 MB\n",
      "\n",
      "Processing year 2004...\n",
      "Found 434 chunk files for year 2004\n",
      "Created consolidated file: consolidated_output\\psc_2004_all_chunks.json\n",
      "File contains 434 chunks\n",
      "File size: 0.41 MB\n",
      "\n",
      "Processing year 2005...\n",
      "Found 437 chunk files for year 2005\n",
      "Created consolidated file: consolidated_output\\psc_2005_all_chunks.json\n",
      "File contains 437 chunks\n",
      "File size: 0.42 MB\n",
      "\n",
      "Processing year 2006...\n",
      "Found 449 chunk files for year 2006\n",
      "Created consolidated file: consolidated_output\\psc_2006_all_chunks.json\n",
      "File contains 449 chunks\n",
      "File size: 0.42 MB\n",
      "\n",
      "Processing year 2007...\n",
      "Found 435 chunk files for year 2007\n",
      "Created consolidated file: consolidated_output\\psc_2007_all_chunks.json\n",
      "File contains 435 chunks\n",
      "File size: 0.40 MB\n",
      "\n",
      "Processing year 2008...\n",
      "Found 445 chunk files for year 2008\n",
      "Created consolidated file: consolidated_output\\psc_2008_all_chunks.json\n",
      "File contains 445 chunks\n",
      "File size: 0.41 MB\n",
      "\n",
      "Processing year 2009...\n",
      "Found 431 chunk files for year 2009\n",
      "Created consolidated file: consolidated_output\\psc_2009_all_chunks.json\n",
      "File contains 431 chunks\n",
      "File size: 0.40 MB\n",
      "\n",
      "Processing year 2010...\n",
      "Found 430 chunk files for year 2010\n",
      "Created consolidated file: consolidated_output\\psc_2010_all_chunks.json\n",
      "File contains 430 chunks\n",
      "File size: 0.41 MB\n",
      "\n",
      "Processing year 2011...\n",
      "Found 433 chunk files for year 2011\n",
      "Created consolidated file: consolidated_output\\psc_2011_all_chunks.json\n",
      "File contains 433 chunks\n",
      "File size: 0.40 MB\n",
      "\n",
      "Processing year 2012...\n",
      "Found 414 chunk files for year 2012\n",
      "Created consolidated file: consolidated_output\\psc_2012_all_chunks.json\n",
      "File contains 414 chunks\n",
      "File size: 0.39 MB\n",
      "\n",
      "Processing year 2013...\n",
      "Found 394 chunk files for year 2013\n",
      "Created consolidated file: consolidated_output\\psc_2013_all_chunks.json\n",
      "File contains 394 chunks\n",
      "File size: 0.37 MB\n",
      "\n",
      "Processing year 2014...\n",
      "Found 276 chunk files for year 2014\n",
      "Created consolidated file: consolidated_output\\psc_2014_all_chunks.json\n",
      "File contains 276 chunks\n",
      "File size: 0.24 MB\n",
      "\n",
      "Consolidation complete! Processed 24034 chunks across 42 years\n",
      "Total size of all consolidated files: 22.14 MB\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4520\\99037248.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[33m\"year\"\u001b[39m: \u001b[33m\"TOTAL\"\u001b[39m,\n\u001b[32m     59\u001b[39m     \u001b[33m\"chunks\"\u001b[39m: df[\u001b[33m\"chunks\"\u001b[39m].sum(),\n\u001b[32m     60\u001b[39m     \u001b[33m\"size_mb\"\u001b[39m: df[\u001b[33m\"size_mb\"\u001b[39m].sum().round(\u001b[32m2\u001b[39m)\n\u001b[32m     61\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m df = df.append(total_row, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m print(\u001b[33m\"Consolidation Results:\"\u001b[39m)\n\u001b[32m     65\u001b[39m print(df)\n",
      "\u001b[32md:\\anaconda3\\envs\\psai\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6295\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6296\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m         ):\n\u001b[32m   6298\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 5: Process All Years\n",
    "def get_available_years():\n",
    "    \"\"\"Get a list of all available years in the chunks directory\"\"\"\n",
    "    year_dirs = glob.glob(os.path.join(CHUNKS_BASE_DIR, \"*\"))\n",
    "    years = []\n",
    "    \n",
    "    for year_dir in year_dirs:\n",
    "        if os.path.isdir(year_dir):\n",
    "            year = os.path.basename(year_dir)\n",
    "            if year.isdigit():  # Make sure it's a year\n",
    "                years.append(year)\n",
    "    \n",
    "    return sorted(years)\n",
    "\n",
    "available_years = get_available_years()\n",
    "print(f\"Found {len(available_years)} years to process: {', '.join(available_years)}\")\n",
    "\n",
    "# Cell 6: Process All Years\n",
    "def process_all_years(years):\n",
    "    \"\"\"Process a list of years\"\"\"\n",
    "    total_chunks = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"\\nProcessing year {year}...\")\n",
    "        \n",
    "        chunks, size = consolidate_year_chunks(year)\n",
    "        \n",
    "        size_mb = size / (1024 * 1024)\n",
    "        results.append({\n",
    "            \"year\": year,\n",
    "            \"chunks\": chunks,\n",
    "            \"size_mb\": size_mb\n",
    "        })\n",
    "        \n",
    "        total_chunks += chunks\n",
    "        total_size += size\n",
    "    \n",
    "    total_size_mb = total_size / (1024 * 1024)\n",
    "    print(f\"\\nConsolidation complete! Processed {total_chunks} chunks across {len(years)} years\")\n",
    "    print(f\"Total size of all consolidated files: {total_size_mb:.2f} MB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process all available years\n",
    "results = process_all_years(available_years)\n",
    "\n",
    "# Cell 7: Display Results Table\n",
    "# Convert results to DataFrame for nice display\n",
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values(\"year\")\n",
    "df[\"size_mb\"] = df[\"size_mb\"].round(2)\n",
    "\n",
    "# Calculate total\n",
    "total_row = {\n",
    "    \"year\": \"TOTAL\",\n",
    "    \"chunks\": df[\"chunks\"].sum(),\n",
    "    \"size_mb\": df[\"size_mb\"].sum().round(2)\n",
    "}\n",
    "df = df.append(total_row, ignore_index=True)\n",
    "\n",
    "print(\"Consolidation Results:\")\n",
    "print(df)\n",
    "\n",
    "# Cell 8: Verify Consolidated Files\n",
    "def verify_consolidated_files(years):\n",
    "    \"\"\"Verify that all expected consolidated files exist and are valid JSON\"\"\"\n",
    "    missing = []\n",
    "    invalid = []\n",
    "    verified = []\n",
    "    \n",
    "    for year in years:\n",
    "        file_path = os.path.join(CONSOLIDATED_DIR, f\"psc_{year}_all_chunks.json\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            missing.append(year)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Basic validation\n",
    "                if \"chunks\" not in data or not isinstance(data[\"chunks\"], list):\n",
    "                    invalid.append(year)\n",
    "                    continue\n",
    "                \n",
    "                chunk_count = len(data[\"chunks\"])\n",
    "                file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                \n",
    "                verified.append({\n",
    "                    \"year\": year,\n",
    "                    \"chunks\": chunk_count,\n",
    "                    \"size_mb\": round(file_size, 2)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying {file_path}: {e}\")\n",
    "            invalid.append(year)\n",
    "    \n",
    "    print(\"\\nVerification Results:\")\n",
    "    print(f\"Verified: {len(verified)} files\")\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"Missing: {len(missing)} files - {', '.join(missing)}\")\n",
    "    \n",
    "    if invalid:\n",
    "        print(f\"Invalid: {len(invalid)} files - {', '.join(invalid)}\")\n",
    "    \n",
    "    return verified, missing, invalid\n",
    "\n",
    "# Verify the consolidated files\n",
    "verified, missing, invalid = verify_consolidated_files(available_years)\n",
    "\n",
    "# Display verified files as a DataFrame\n",
    "if verified:\n",
    "    verify_df = pd.DataFrame(verified)\n",
    "    verify_df = verify_df.sort_values(\"year\")\n",
    "    print(\"\\nVerified Files:\")\n",
    "    print(verify_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6e50eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pandas is already installed\n",
      " openpyxl is already installed\n",
      "Loading metadata from: D:\\Technical_projects\\PSAI\\raw_data\\PSC\\000_LIMBFILES\\1970s.xlsx\n",
      "Columns in file: ['Category', 'media relative path', 'file name', 'title', 'subject', 'creator', 'date', 'publication', 'collection', 'series', 'physical location', 'notes', 'format', 'clearance']\n",
      "Loading metadata from: D:\\Technical_projects\\PSAI\\raw_data\\PSC\\000_LIMBFILES\\80s.xlsx\n",
      "Columns in file: ['Category', 'media relative path', 'file name', 'title', 'subject', 'creator', 'date', 'publication', 'collection', 'series', 'physical location', 'notes', 'format', 'clearance']\n",
      "Loading metadata from: D:\\Technical_projects\\PSAI\\raw_data\\PSC\\000_LIMBFILES\\90s.csv\n",
      "Detected encoding for 90s.csv: ascii (confidence: 1.00)\n",
      "Columns in file: ['Category', 'media relative path', 'file name', 'title', 'subject', 'creator', 'date', 'publication', 'collection', 'series', 'physical location', 'notes', 'format', 'clearance']\n",
      "Loading metadata from: D:\\Technical_projects\\PSAI\\raw_data\\PSC\\000_LIMBFILES\\00s.csv\n",
      "Detected encoding for 00s.csv: Windows-1252 (confidence: 0.73)\n",
      "Columns in file: ['Category', 'media relative path', 'file name', 'title', 'subject', 'creator', 'date', 'publication', 'collection', 'series', 'physical location', 'notes', 'format', 'clearance']\n",
      "Loaded metadata for 2693 documents\n",
      "\n",
      "Sample metadata entries:\n",
      "  PSC_1974_01_01: {'title': 'National Protection Act', 'subjects': ['Congress']}\n",
      "  PSC_1974_01_04: {'title': 'Blame for Gasoline Shortage', 'subjects': ['Economy']}\n",
      "  PSC_1974_01_08: {'title': 'Legal Services', 'subjects': ['Law and Order']}\n",
      "  PSC_1974_01_11: {'title': \"Sozhenitsyn's New Book\", 'subjects': ['Literature', 'Communism']}\n",
      "  PSC_1974_01_15: {'title': 'Solving the Oil Crisis', 'subjects': ['Economy']}\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1973_all_chunks.json\n",
      "Found 38 unique source files in 218 chunks\n",
      "Sample source files:\n",
      "  - PSC_1973_11_30.pdf\n",
      "  - PSC_1973_12_28.pdf\n",
      "  - PSR 1973-11-23.docx\n",
      "  - PSC_1973_10_30.pdf\n",
      "  - PSC_1973_10_12.pdf\n",
      "Updated 0/218 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1973_all_chunks.json\n",
      "Matched 0 unique files, failed to match 38 files\n",
      "Examples of unmatched files:\n",
      "  - PSC_1973_11_30.pdf\n",
      "  - PSC_1973_12_28.pdf\n",
      "  - PSC_1973_10_30.pdf\n",
      "  - PSR 1973-11-23.docx\n",
      "  - PSC_1973_10_12.pdf\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1974_all_chunks.json\n",
      "Found 186 unique source files in 1043 chunks\n",
      "Sample source files:\n",
      "  - PSR 1974-09-10.docx\n",
      "  - PSR 1974-01-08.docx\n",
      "  - PSR 1974-08-30.docx\n",
      "  - PSR 1974-02-8.docx\n",
      "  - PSC_1974_04_12.pdf\n",
      "Updated 1043/1043 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1974_all_chunks.json\n",
      "Matched 186 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1975_all_chunks.json\n",
      "Found 104 unique source files in 587 chunks\n",
      "Sample source files:\n",
      "  - PSC_1975_02_21.docx\n",
      "  - PSC_1975_02_25.docx\n",
      "  - PSC_1975_10_14.docx\n",
      "  - PSC_1975_04_04.docx\n",
      "  - PSC_1975_02_14.docx\n",
      "Updated 587/587 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1975_all_chunks.json\n",
      "Matched 104 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1976_all_chunks.json\n",
      "Found 106 unique source files in 623 chunks\n",
      "Sample source files:\n",
      "  - PSC_1976_08_17.docx\n",
      "  - PSC_1976_02_17.docx\n",
      "  - PSC_1976_07_06.docx\n",
      "  - PSC_1976_08_03.docx\n",
      "  - PSC_1976_05_25.docx\n",
      "Updated 623/623 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1976_all_chunks.json\n",
      "Matched 106 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1977_all_chunks.json\n",
      "Found 102 unique source files in 671 chunks\n",
      "Sample source files:\n",
      "  - PSC_1977_07_19.docx\n",
      "  - PSC_1977_06_03.docx\n",
      "  - PSC_1977_05_10.docx\n",
      "  - PSC_1977_03_08.docx\n",
      "  - PSC_1977_09_06.docx\n",
      "Updated 671/671 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1977_all_chunks.json\n",
      "Matched 102 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1978_all_chunks.json\n",
      "Found 103 unique source files in 736 chunks\n",
      "Sample source files:\n",
      "  - PSC_1978_12_12.docx\n",
      "  - PSC_1978_12_29.docx\n",
      "  - PSC_1978_04_21.docx\n",
      "  - PSC_1978_10_10.docx\n",
      "  - PSC_1978_06_30.docx\n",
      "Updated 736/736 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1978_all_chunks.json\n",
      "Matched 103 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1979_all_chunks.json\n",
      "Found 102 unique source files in 764 chunks\n",
      "Sample source files:\n",
      "  - PSC_1979_09_07.pdf\n",
      "  - PSC_1979_01_09.pdf\n",
      "  - PSC_1979_02_09.pdf\n",
      "  - PSC_1979_08_24.pdf\n",
      "  - PSC_1979_07_24.pdf\n",
      "Updated 764/764 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1979_all_chunks.json\n",
      "Matched 102 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1980_all_chunks.json\n",
      "Found 104 unique source files in 817 chunks\n",
      "Sample source files:\n",
      "  - PSC_1980_06_03.pdf\n",
      "  - PSC_1980_12_05.pdf\n",
      "  - PSC_1980_08_29.pdf\n",
      "  - PSC_1980_04_01.pdf\n",
      "  - PSC_1980_05_23.pdf\n",
      "Updated 817/817 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1980_all_chunks.json\n",
      "Matched 104 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1981_all_chunks.json\n",
      "Found 104 unique source files in 829 chunks\n",
      "Sample source files:\n",
      "  - PSC_1981_02_24.pdf\n",
      "  - PSC_1981_10_23.pdf\n",
      "  - PSC_1981_09_15.pdf\n",
      "  - PSC_1981_06_02.pdf\n",
      "  - PSC_1981_10_20.pdf\n",
      "Updated 829/829 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1981_all_chunks.json\n",
      "Matched 104 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1982_all_chunks.json\n",
      "Found 105 unique source files in 843 chunks\n",
      "Sample source files:\n",
      "  - PSC_1982_04_06.pdf\n",
      "  - PSC_1982_01_05.pdf\n",
      "  - PSC_1982_07_09.pdf\n",
      "  - PSC_1982_11_16.pdf\n",
      "  - PSC_1982_10_22.pdf\n",
      "Updated 843/843 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1982_all_chunks.json\n",
      "Matched 105 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1983_all_chunks.json\n",
      "Found 104 unique source files in 1392 chunks\n",
      "Sample source files:\n",
      "  - PSC_1983_12_27.pdf\n",
      "  - PSC_1983_01_28.pdf\n",
      "  - PSC_1983_05_20.pdf\n",
      "  - PSC_1983_12_23.pdf\n",
      "  - PSC_1983_12_20.pdf\n",
      "Updated 1392/1392 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1983_all_chunks.json\n",
      "Matched 104 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1984_all_chunks.json\n",
      "Found 104 unique source files in 793 chunks\n",
      "Sample source files:\n",
      "  - PSC_1984_11_09.pdf\n",
      "  - PSC_1984_02_03.pdf\n",
      "  - PSC_1984_10_19.pdf\n",
      "  - PSC_1984_06_12.pdf\n",
      "  - PSC_1984_06_01.pdf\n",
      "Updated 793/793 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1984_all_chunks.json\n",
      "Matched 104 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1985_all_chunks.json\n",
      "Found 107 unique source files in 802 chunks\n",
      "Sample source files:\n",
      "  - PSC_1985_05_30 PS2.pdf\n",
      "  - PSC_1985_05_03.pdf\n",
      "  - PSC_1985_10_03 PS1.pdf\n",
      "  - PSC_1985_11_28 PS2.pdf\n",
      "  - PSC_1985_12_05 PS1.pdf\n",
      "Updated 802/802 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1985_all_chunks.json\n",
      "Matched 107 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1986_all_chunks.json\n",
      "Found 106 unique source files in 820 chunks\n",
      "Sample source files:\n",
      "  - PSC_1986_03_06 PS1.pdf\n",
      "  - PSC_1986_01_30 PS2.pdf\n",
      "  - PSC_1986_12_11 PS1.pdf\n",
      "  - PSC_1986_03_13 PS1.pdf\n",
      "  - PSC_1986_05_08 PS2.pdf\n",
      "Updated 820/820 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1986_all_chunks.json\n",
      "Matched 106 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1987_all_chunks.json\n",
      "Found 107 unique source files in 824 chunks\n",
      "Sample source files:\n",
      "  - PSC_1987_02_12 PS2.pdf\n",
      "  - PSC_1987_02_19 PS1.pdf\n",
      "  - PSC_1987_06_25 PS1.pdf\n",
      "  - PSC_1987_12_10 PS2.pdf\n",
      "  - PSC_1987_05_14 PS1.pdf\n",
      "Updated 824/824 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1987_all_chunks.json\n",
      "Matched 107 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1988_all_chunks.json\n",
      "Found 106 unique source files in 810 chunks\n",
      "Sample source files:\n",
      "  - PSC_1988_10_06 PS1.docx\n",
      "  - PSC_1988_01_21 PS2.pdf\n",
      "  - PSC_1988_03_31 PS1.pdf\n",
      "  - PSC_1988_05_26 PS1.pdf\n",
      "  - PSC_1988_04_28 PS2.pdf\n",
      "Updated 810/810 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1988_all_chunks.json\n",
      "Matched 106 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1989_all_chunks.json\n",
      "Found 83 unique source files in 675 chunks\n",
      "Sample source files:\n",
      "  - PSC_1989_09_14.pdf\n",
      "  - PSC_1989_02_02 PS1.docx\n",
      "  - PSC_1989_03_23 PS1.docx\n",
      "  - PSC_1989_08_24.docx\n",
      "  - PSC_1989_06_01 PS1.docx\n",
      "Updated 675/675 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1989_all_chunks.json\n",
      "Matched 83 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1990_all_chunks.json\n",
      "Found 52 unique source files in 445 chunks\n",
      "Sample source files:\n",
      "  - PSC_1990_03_08.pdf\n",
      "  - PSC_1990_11_15.pdf\n",
      "  - PSC_1990_03_22.pdf\n",
      "  - PSC_1990_09_13.pdf\n",
      "  - PSC_1990_02_08.pdf\n",
      "Updated 445/445 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1990_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1991_all_chunks.json\n",
      "Found 53 unique source files in 431 chunks\n",
      "Sample source files:\n",
      "  - PSC_1991_05_23.pdf\n",
      "  - PSC_1991_12_19.pdf\n",
      "  - PSC_1991_01_01.pdf\n",
      "  - PSC_1991_06_20.pdf\n",
      "  - PSC_1991_05_16.pdf\n",
      "Updated 431/431 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1991_all_chunks.json\n",
      "Matched 53 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1992_all_chunks.json\n",
      "Found 53 unique source files in 452 chunks\n",
      "Sample source files:\n",
      "  - PSC_1992_06_04.pdf\n",
      "  - PSC_1992_01_02.pdf\n",
      "  - PSC_1992_10_08.pdf\n",
      "  - PSC_1992_12_17.pdf\n",
      "  - PSC_1992_08_13.pdf\n",
      "Updated 452/452 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1992_all_chunks.json\n",
      "Matched 53 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1993_all_chunks.json\n",
      "Found 52 unique source files in 433 chunks\n",
      "Sample source files:\n",
      "  - PSC_1993_08_26.docx\n",
      "  - PSC_1993_05_13.docx\n",
      "  - PSC_1993_09_30.docx\n",
      "  - PSC_1993_06_03.docx\n",
      "  - PSC_1993_02_04.pdf\n",
      "Updated 433/433 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1993_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1994_all_chunks.json\n",
      "Found 55 unique source files in 430 chunks\n",
      "Sample source files:\n",
      "  - PSC_1994_03_10.docx\n",
      "  - PSC_1994_03_31.docx\n",
      "  - PSC_1994_09_15.docx\n",
      "  - PSC_1994_11_17.docx\n",
      "  - PSC_1994_12_28.docx\n",
      "Updated 430/430 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1994_all_chunks.json\n",
      "Matched 55 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1995_all_chunks.json\n",
      "Found 52 unique source files in 433 chunks\n",
      "Sample source files:\n",
      "  - PSC_1995_04_19.docx\n",
      "  - PSC_1995_09_14.docx\n",
      "  - PSC_1995_04_13.docx\n",
      "  - PSC_1995_07_27.docx\n",
      "  - PSC_1995_02_02.docx\n",
      "Updated 433/433 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1995_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1996_all_chunks.json\n",
      "Found 51 unique source files in 434 chunks\n",
      "Sample source files:\n",
      "  - PSC_1996_01_01.pdf\n",
      "  - PSC_1996_06_27.docx\n",
      "  - PSC_1996_10_24.docx\n",
      "  - PSC_1996_02_15.docx\n",
      "  - PSC_1996_10_10.docx\n",
      "Updated 417/434 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1996_all_chunks.json\n",
      "Matched 49 unique files, failed to match 2 files\n",
      "Examples of unmatched files:\n",
      "  - PSC_1966_09_19.docx\n",
      "  - PSC_1966_11_27.docx\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1997_all_chunks.json\n",
      "Found 53 unique source files in 461 chunks\n",
      "Sample source files:\n",
      "  - PSC_1997_01_15.docx\n",
      "  - PSC_1997_04_02.docx\n",
      "  - PSC_1997_03_19.docx\n",
      "  - PSC_1997_10_01.docx\n",
      "  - PSC_1997_08_06.docx\n",
      "Updated 461/461 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1997_all_chunks.json\n",
      "Matched 53 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1998_all_chunks.json\n",
      "Found 52 unique source files in 456 chunks\n",
      "Sample source files:\n",
      "  - PSC_1998_01_21.docx\n",
      "  - PSC_1998_07_15.docx\n",
      "  - PSC_1998_02_18.docx\n",
      "  - PSC_1998_04_08.docx\n",
      "  - PSC_1998_07_29.pdf\n",
      "Updated 456/456 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1998_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1999_all_chunks.json\n",
      "Found 52 unique source files in 448 chunks\n",
      "Sample source files:\n",
      "  - PSC_1999_05_26.docx\n",
      "  - PSC_1999_05_05.docx\n",
      "  - PSC_1999_01_06.docx\n",
      "  - PSC_1999_03_10.docx\n",
      "  - PSC_1999_09_01.docx\n",
      "Updated 448/448 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_1999_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2000_all_chunks.json\n",
      "Found 52 unique source files in 458 chunks\n",
      "Sample source files:\n",
      "  - PSC_2000_07_05.docx\n",
      "  - PSC_2000_04_19.docx\n",
      "  - PSC_2000_08_16.docx\n",
      "  - PSC_2000_08_23.docx\n",
      "  - PSC_2000_08_30.docx\n",
      "Updated 458/458 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2000_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2001_all_chunks.json\n",
      "Found 52 unique source files in 428 chunks\n",
      "Sample source files:\n",
      "  - PSC_2001_08_29.docx\n",
      "  - PSC_2001_10_17.docx\n",
      "  - PSC_2001_09_19.docx\n",
      "  - PSC_2001_11_28.docx\n",
      "  - PSC_2001_08_01.docx\n",
      "Updated 428/428 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2001_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2002_all_chunks.json\n",
      "Found 52 unique source files in 441 chunks\n",
      "Sample source files:\n",
      "  - PSC_2002_04_10.docx\n",
      "  - PSC_2002_01_30.docx\n",
      "  - PSC_2002_10_30.docx\n",
      "  - PSC_2002_05_15.docx\n",
      "  - PSC_2002_06_26.docx\n",
      "Updated 441/441 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2002_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2003_all_chunks.json\n",
      "Found 53 unique source files in 459 chunks\n",
      "Sample source files:\n",
      "  - PSC_2003_09_17.docx\n",
      "  - PSC_2003_12_17.docx\n",
      "  - PSC_2003_08_27.docx\n",
      "  - PSC_2003_10_01.docx\n",
      "  - PSC_2003_07_16.docx\n",
      "Updated 459/459 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2003_all_chunks.json\n",
      "Matched 53 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2004_all_chunks.json\n",
      "Found 52 unique source files in 434 chunks\n",
      "Sample source files:\n",
      "  - PSC_2004_09_29.docx\n",
      "  - PSC_2004_10_06.docx\n",
      "  - PSC_2004_02_11.docx\n",
      "  - PSC_2004_08_25.docx\n",
      "  - PSC_2004_07_21.docx\n",
      "Updated 434/434 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2004_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2005_all_chunks.json\n",
      "Found 53 unique source files in 437 chunks\n",
      "Sample source files:\n",
      "  - PSC_2005_01_26.docx\n",
      "  - PSC_2005_08_17.pdf\n",
      "  - PSC_2005_09_14.docx\n",
      "  - PSC_2005_02_16.docx\n",
      "  - PSC_2005_12_21.docx\n",
      "Updated 436/437 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2005_all_chunks.json\n",
      "Matched 52 unique files, failed to match 1 files\n",
      "Examples of unmatched files:\n",
      "  - ~$c011905.docx\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2006_all_chunks.json\n",
      "Found 52 unique source files in 449 chunks\n",
      "Sample source files:\n",
      "  - PSC_2006_01_06.docx\n",
      "  - PSC_2006_03_08.docx\n",
      "  - PSC_2006_08_02.docx\n",
      "  - PSC_2006_10_11.docx\n",
      "  - PSC_2006_04_26.docx\n",
      "Updated 449/449 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2006_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2007_all_chunks.json\n",
      "Found 52 unique source files in 435 chunks\n",
      "Sample source files:\n",
      "  - PSC_2007_01_17.docx\n",
      "  - PSC_2007_03_14.docx\n",
      "  - PSC_2007_05_23.docx\n",
      "  - PSC_2007_02_14.docx\n",
      "  - PSC_2007_02_28.docx\n",
      "Updated 435/435 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2007_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2008_all_chunks.json\n",
      "Found 53 unique source files in 445 chunks\n",
      "Sample source files:\n",
      "  - PSC_2008_03_26.docx\n",
      "  - PSC_2008_12_26.docx\n",
      "  - PSC_2008_07_16.docx\n",
      "  - PSC_2008_04_30.docx\n",
      "  - PSC_2008_11_07.docx\n",
      "Updated 445/445 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2008_all_chunks.json\n",
      "Matched 53 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2009_all_chunks.json\n",
      "Found 52 unique source files in 431 chunks\n",
      "Sample source files:\n",
      "  - PSC_2009_05_08.docx\n",
      "  - PSC_2009_03_13.docx\n",
      "  - PSC_2009_01_02.docx\n",
      "  - PSC_2009_03_06.docx\n",
      "  - PSC_2009_06_05.docx\n",
      "Updated 431/431 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2009_all_chunks.json\n",
      "Matched 52 unique files, failed to match 0 files\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2010_all_chunks.json\n",
      "Found 53 unique source files in 430 chunks\n",
      "Sample source files:\n",
      "  - PSC_2010_04_16.docx\n",
      "  - PSC_2010_05_14.docx\n",
      "  - PSC_2010_07_16.docx\n",
      "  - PSC_2010_11_19.docx\n",
      "  - PSC_2010_02_05.docx\n",
      "Updated 0/430 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2010_all_chunks.json\n",
      "Matched 0 unique files, failed to match 53 files\n",
      "Examples of unmatched files:\n",
      "  - PSC_2010_04_16.docx\n",
      "  - PSC_2010_05_14.docx\n",
      "  - PSC_2010_07_16.docx\n",
      "  - PSC_2010_11_19.docx\n",
      "  - PSC_2010_02_05.docx\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2011_all_chunks.json\n",
      "Found 52 unique source files in 433 chunks\n",
      "Sample source files:\n",
      "  - PSC_2011_06_10.docx\n",
      "  - PSC_2011_10_05.docx\n",
      "  - PSC_2011_10_26.docx\n",
      "  - PSC_2011_09_14.docx\n",
      "  - PSC_2011_07_15.docx\n",
      "Updated 0/433 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2011_all_chunks.json\n",
      "Matched 0 unique files, failed to match 52 files\n",
      "Examples of unmatched files:\n",
      "  - PSC_2011_10_05.docx\n",
      "  - PSC_2011_06_10.docx\n",
      "  - PSC_2011_10_26.docx\n",
      "  - PSC_2011_09_02.docx\n",
      "  - PSC_2011_07_15.docx\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2012_all_chunks.json\n",
      "Found 52 unique source files in 414 chunks\n",
      "Sample source files:\n",
      "  - PSC_2012_05_02.docx\n",
      "  - PSC_2012_03_21.docx\n",
      "  - PSC_2012_11_28.docx\n",
      "  - PSC_2012_01_25.docx\n",
      "  - PSC_2012_01_11.docx\n",
      "Updated 0/414 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2012_all_chunks.json\n",
      "Matched 0 unique files, failed to match 52 files\n",
      "Examples of unmatched files:\n",
      "  - PSC_2012_03_21.docx\n",
      "  - PSC_2012_06_27.docx\n",
      "  - PSC_2012_11_28.docx\n",
      "  - PSC_2012_01_25.docx\n",
      "  - PSC_2012_01_11.docx\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2013_all_chunks.json\n",
      "Found 54 unique source files in 394 chunks\n",
      "Sample source files:\n",
      "  - PSC_2013_09_25.docx\n",
      "  - PSC_2013_03_27.docx\n",
      "  - Template.docx\n",
      "  - PSC_2013_10_02.docx\n",
      "  - PSC_2013_06_19.docx\n",
      "Updated 0/394 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2013_all_chunks.json\n",
      "Matched 0 unique files, failed to match 54 files\n",
      "Examples of unmatched files:\n",
      "  - PSC_2013_09_25.docx\n",
      "  - PSC_2013_03_27.docx\n",
      "  - Template.docx\n",
      "  - PSC_2013_10_02.docx\n",
      "  - PSC_2013_06_19.docx\n",
      "Updating file: D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2014_all_chunks.json\n",
      "Found 51 unique source files in 276 chunks\n",
      "Sample source files:\n",
      "  - PSC_2014_02_18.docx\n",
      "  - PSC_2014_03_11.docx\n",
      "  - PSC_2014_07_02.docx\n",
      "  - PSC_2014_06_10.docx\n",
      "  - PSC_2014_08_26.doc\n",
      "Updated 0/276 chunks in D:\\Technical_projects\\PSAI\\code\\consolidated_output\\psc_2014_all_chunks.json\n",
      "Matched 0 unique files, failed to match 51 files\n",
      "Examples of unmatched files:\n",
      "  - PSC_2014_02_18.docx\n",
      "  - PSC_2014_03_11.docx\n",
      "  - PSC_2014_07_02.docx\n",
      "  - PSC_2014_06_10.docx\n",
      "  - PSC_2014_08_26.doc\n",
      "\n",
      "Update results by year:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>File</th>\n",
       "      <th>Updated Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1973</td>\n",
       "      <td>psc_1973_all_chunks.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1974</td>\n",
       "      <td>psc_1974_all_chunks.json</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1975</td>\n",
       "      <td>psc_1975_all_chunks.json</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>psc_1976_all_chunks.json</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1977</td>\n",
       "      <td>psc_1977_all_chunks.json</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1978</td>\n",
       "      <td>psc_1978_all_chunks.json</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1979</td>\n",
       "      <td>psc_1979_all_chunks.json</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1980</td>\n",
       "      <td>psc_1980_all_chunks.json</td>\n",
       "      <td>817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1981</td>\n",
       "      <td>psc_1981_all_chunks.json</td>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1982</td>\n",
       "      <td>psc_1982_all_chunks.json</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1983</td>\n",
       "      <td>psc_1983_all_chunks.json</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1984</td>\n",
       "      <td>psc_1984_all_chunks.json</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1985</td>\n",
       "      <td>psc_1985_all_chunks.json</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1986</td>\n",
       "      <td>psc_1986_all_chunks.json</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1987</td>\n",
       "      <td>psc_1987_all_chunks.json</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1988</td>\n",
       "      <td>psc_1988_all_chunks.json</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1989</td>\n",
       "      <td>psc_1989_all_chunks.json</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1990</td>\n",
       "      <td>psc_1990_all_chunks.json</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1991</td>\n",
       "      <td>psc_1991_all_chunks.json</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1992</td>\n",
       "      <td>psc_1992_all_chunks.json</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1993</td>\n",
       "      <td>psc_1993_all_chunks.json</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1994</td>\n",
       "      <td>psc_1994_all_chunks.json</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1995</td>\n",
       "      <td>psc_1995_all_chunks.json</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1996</td>\n",
       "      <td>psc_1996_all_chunks.json</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1997</td>\n",
       "      <td>psc_1997_all_chunks.json</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1998</td>\n",
       "      <td>psc_1998_all_chunks.json</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1999</td>\n",
       "      <td>psc_1999_all_chunks.json</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2000</td>\n",
       "      <td>psc_2000_all_chunks.json</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2001</td>\n",
       "      <td>psc_2001_all_chunks.json</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2002</td>\n",
       "      <td>psc_2002_all_chunks.json</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2003</td>\n",
       "      <td>psc_2003_all_chunks.json</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2004</td>\n",
       "      <td>psc_2004_all_chunks.json</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2005</td>\n",
       "      <td>psc_2005_all_chunks.json</td>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2006</td>\n",
       "      <td>psc_2006_all_chunks.json</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2007</td>\n",
       "      <td>psc_2007_all_chunks.json</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2008</td>\n",
       "      <td>psc_2008_all_chunks.json</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2009</td>\n",
       "      <td>psc_2009_all_chunks.json</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2010</td>\n",
       "      <td>psc_2010_all_chunks.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2011</td>\n",
       "      <td>psc_2011_all_chunks.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2012</td>\n",
       "      <td>psc_2012_all_chunks.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2013</td>\n",
       "      <td>psc_2013_all_chunks.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2014</td>\n",
       "      <td>psc_2014_all_chunks.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year                      File  Updated Chunks\n",
       "0   1973  psc_1973_all_chunks.json               0\n",
       "1   1974  psc_1974_all_chunks.json            1043\n",
       "2   1975  psc_1975_all_chunks.json             587\n",
       "3   1976  psc_1976_all_chunks.json             623\n",
       "4   1977  psc_1977_all_chunks.json             671\n",
       "5   1978  psc_1978_all_chunks.json             736\n",
       "6   1979  psc_1979_all_chunks.json             764\n",
       "7   1980  psc_1980_all_chunks.json             817\n",
       "8   1981  psc_1981_all_chunks.json             829\n",
       "9   1982  psc_1982_all_chunks.json             843\n",
       "10  1983  psc_1983_all_chunks.json            1392\n",
       "11  1984  psc_1984_all_chunks.json             793\n",
       "12  1985  psc_1985_all_chunks.json             802\n",
       "13  1986  psc_1986_all_chunks.json             820\n",
       "14  1987  psc_1987_all_chunks.json             824\n",
       "15  1988  psc_1988_all_chunks.json             810\n",
       "16  1989  psc_1989_all_chunks.json             675\n",
       "17  1990  psc_1990_all_chunks.json             445\n",
       "18  1991  psc_1991_all_chunks.json             431\n",
       "19  1992  psc_1992_all_chunks.json             452\n",
       "20  1993  psc_1993_all_chunks.json             433\n",
       "21  1994  psc_1994_all_chunks.json             430\n",
       "22  1995  psc_1995_all_chunks.json             433\n",
       "23  1996  psc_1996_all_chunks.json             417\n",
       "24  1997  psc_1997_all_chunks.json             461\n",
       "25  1998  psc_1998_all_chunks.json             456\n",
       "26  1999  psc_1999_all_chunks.json             448\n",
       "27  2000  psc_2000_all_chunks.json             458\n",
       "28  2001  psc_2001_all_chunks.json             428\n",
       "29  2002  psc_2002_all_chunks.json             441\n",
       "30  2003  psc_2003_all_chunks.json             459\n",
       "31  2004  psc_2004_all_chunks.json             434\n",
       "32  2005  psc_2005_all_chunks.json             436\n",
       "33  2006  psc_2006_all_chunks.json             449\n",
       "34  2007  psc_2007_all_chunks.json             435\n",
       "35  2008  psc_2008_all_chunks.json             445\n",
       "36  2009  psc_2009_all_chunks.json             431\n",
       "37  2010  psc_2010_all_chunks.json               0\n",
       "38  2011  psc_2011_all_chunks.json               0\n",
       "39  2012  psc_2012_all_chunks.json               0\n",
       "40  2013  psc_2013_all_chunks.json               0\n",
       "41  2014  psc_2014_all_chunks.json               0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Install required dependencies\n",
    "# Run this cell first to ensure all dependencies are installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"Install required packages if they're not already installed\"\"\"\n",
    "    required_packages = ['pandas', 'openpyxl']\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\" {package} is already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\" {package} has been installed\")\n",
    "\n",
    "# Install required dependencies\n",
    "install_dependencies()\n",
    "\n",
    "# Cell 2: Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet  # For detecting file encoding\n",
    "\n",
    "# Cell 3: Define the paths to your files\n",
    "# Update these paths to match your environment\n",
    "CONSOLIDATED_DIR = r\"D:\\Technical_projects\\PSAI\\code\\consolidated_output\"  # Directory with your consolidated JSON files\n",
    "METADATA_FILES = [\n",
    "    r\"D:\\Technical_projects\\PSAI\\raw_data\\PSC\\000_LIMBFILES\\1970s.xlsx\",\n",
    "    r\"D:\\Technical_projects\\PSAI\\raw_data\\PSC\\000_LIMBFILES\\80s.xlsx\",\n",
    "    r\"D:\\Technical_projects\\PSAI\\raw_data\\PSC\\000_LIMBFILES\\90s.csv\",\n",
    "    r\"D:\\Technical_projects\\PSAI\\raw_data\\PSC\\000_LIMBFILES\\00s.csv\"\n",
    "]\n",
    "\n",
    "# Cell 4: Define function to detect file encoding\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Detect the encoding of a file using chardet\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        str: Detected encoding\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Install chardet if not present\n",
    "        try:\n",
    "            import chardet\n",
    "        except ImportError:\n",
    "            print(\"Installing chardet package...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"chardet\"])\n",
    "            import chardet\n",
    "            print(\"chardet installed successfully\")\n",
    "        \n",
    "        # Read a sample of the file to detect encoding\n",
    "        with open(file_path, 'rb') as f:\n",
    "            # Read up to 1MB of the file\n",
    "            raw_data = f.read(1024 * 1024)\n",
    "        \n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "        confidence = result['confidence']\n",
    "        \n",
    "        print(f\"Detected encoding for {os.path.basename(file_path)}: {encoding} (confidence: {confidence:.2f})\")\n",
    "        \n",
    "        # If encoding is None or confidence is low, use a common fallback\n",
    "        if not encoding or confidence < 0.7:\n",
    "            print(f\"Low confidence in encoding detection, trying common encodings\")\n",
    "            # Try common encodings\n",
    "            for enc in ['utf-8', 'latin1', 'iso-8859-1', 'cp1252', 'utf-16']:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=enc) as f:\n",
    "                        # Try to read a few lines\n",
    "                        for _ in range(5):\n",
    "                            f.readline()\n",
    "                    print(f\"Successfully read with {enc} encoding\")\n",
    "                    return enc\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            # If all else fails, use latin1 which rarely fails\n",
    "            print(\"Falling back to latin1 encoding\")\n",
    "            return 'latin1'\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting encoding: {e}\")\n",
    "        print(\"Falling back to latin1 encoding\")\n",
    "        return 'latin1'  # Fallback to latin1 which rarely fails\n",
    "\n",
    "# Cell 5: Define functions for loading metadata and updating JSON files\n",
    "def load_metadata_from_excel_csv(file_paths):\n",
    "    \"\"\"\n",
    "    Load metadata from Excel and CSV files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): List of paths to Excel and CSV files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping filenames to their metadata (title and subjects)\n",
    "    \"\"\"\n",
    "    metadata_map = {}\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Loading metadata from: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load file based on extension\n",
    "            if file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "                try:\n",
    "                    # First try with default engine\n",
    "                    df = pd.read_excel(file_path)\n",
    "                except ImportError as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    print(\"Installing openpyxl...\")\n",
    "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openpyxl\"])\n",
    "                    try:\n",
    "                        df = pd.read_excel(file_path)\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Error loading Excel file after installing openpyxl: {e2}\")\n",
    "                        print(\"Converting Excel to CSV format...\")\n",
    "                        # Try to use alternative method\n",
    "                        try:\n",
    "                            import subprocess\n",
    "                            temp_csv = file_path + \".temp.csv\"\n",
    "                            cmd = [sys.executable, \"-c\", \n",
    "                                f\"import pandas as pd; pd.read_excel('{file_path}').to_csv('{temp_csv}', index=False)\"]\n",
    "                            subprocess.check_call(cmd)\n",
    "                            \n",
    "                            # Detect encoding for the CSV\n",
    "                            encoding = detect_encoding(temp_csv)\n",
    "                            df = pd.read_csv(temp_csv, encoding=encoding)\n",
    "                            \n",
    "                            # Clean up temporary file\n",
    "                            os.remove(temp_csv)\n",
    "                        except Exception as e3:\n",
    "                            print(f\"Failed to convert Excel to CSV: {e3}\")\n",
    "                            print(f\"Skipping file {file_path}\")\n",
    "                            continue\n",
    "                            \n",
    "            elif file_path.endswith('.csv'):\n",
    "                # Detect the encoding of the CSV file\n",
    "                encoding = detect_encoding(file_path)\n",
    "                \n",
    "                # Try to read with detected encoding\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=encoding)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading CSV with detected encoding: {e}\")\n",
    "                    # Try alternative encodings if the detected one fails\n",
    "                    for alt_encoding in ['latin1', 'cp1252', 'iso-8859-1', 'utf-16']:\n",
    "                        if alt_encoding != encoding:\n",
    "                            try:\n",
    "                                print(f\"Trying alternate encoding: {alt_encoding}\")\n",
    "                                df = pd.read_csv(file_path, encoding=alt_encoding)\n",
    "                                print(f\"Successfully read with {alt_encoding}\")\n",
    "                                break\n",
    "                            except Exception:\n",
    "                                continue\n",
    "                    else:\n",
    "                        print(f\"Failed to read CSV with any encoding. Skipping file: {file_path}\")\n",
    "                        continue\n",
    "            else:\n",
    "                print(f\"Unsupported file format: {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Check if we have the necessary columns\n",
    "            required_columns = ['file name', 'title']\n",
    "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "            \n",
    "            if missing_columns:\n",
    "                print(f\"Warning: Missing required columns in {file_path}: {missing_columns}\")\n",
    "                # Try to find alternative column names\n",
    "                column_mapping = {}\n",
    "                for req_col in missing_columns:\n",
    "                    # Look for similar columns\n",
    "                    for col in df.columns:\n",
    "                        if req_col.lower() in col.lower():\n",
    "                            column_mapping[col] = req_col\n",
    "                            print(f\"Using '{col}' as '{req_col}'\")\n",
    "                            break\n",
    "                \n",
    "                # Rename columns if alternatives were found\n",
    "                if column_mapping:\n",
    "                    df = df.rename(columns=column_mapping)\n",
    "                \n",
    "                # Check again for required columns\n",
    "                still_missing = [col for col in required_columns if col not in df.columns]\n",
    "                if still_missing:\n",
    "                    print(f\"Still missing required columns: {still_missing}. Skipping file.\")\n",
    "                    continue\n",
    "            \n",
    "            # Print column information\n",
    "            print(f\"Columns in file: {list(df.columns)}\")\n",
    "            \n",
    "            # Process each row\n",
    "            for i, row in df.iterrows():\n",
    "                # Get filename, title, and subject\n",
    "                try:\n",
    "                    # Try to handle potential NaN or missing values\n",
    "                    file_name = row.get('file name')\n",
    "                    if pd.isna(file_name):\n",
    "                        continue\n",
    "                        \n",
    "                    title = row.get('title')\n",
    "                    if pd.isna(title):\n",
    "                        continue\n",
    "                    \n",
    "                    # Look for subject column with various possible names\n",
    "                    subject = None\n",
    "                    for subject_col in ['subject', 'subjects', 'Subject', 'Subjects', 'SUBJECT', 'SUBJECTS']:\n",
    "                        if subject_col in row:\n",
    "                            subject = row.get(subject_col)\n",
    "                            break\n",
    "                    \n",
    "                    # Skip if any essential field is missing or NaN\n",
    "                    if not file_name or not title or pd.isna(file_name) or pd.isna(title):\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert to string and clean up\n",
    "                    file_name = str(file_name).strip().strip('\"\\'')\n",
    "                    title = str(title).strip()\n",
    "                    \n",
    "                    # Extract the base filename without extension\n",
    "                    base_name = os.path.splitext(file_name)[0]\n",
    "                    \n",
    "                    # Split subjects if they contain semicolons or commas\n",
    "                    subject_list = []\n",
    "                    if subject and not pd.isna(subject):\n",
    "                        # Convert to string in case it's a different type\n",
    "                        subject = str(subject)\n",
    "                        # Handle both semicolon and comma separators\n",
    "                        if ';' in subject:\n",
    "                            subject_list = [s.strip() for s in subject.split(';')]\n",
    "                        elif ',' in subject:\n",
    "                            subject_list = [s.strip() for s in subject.split(',')]\n",
    "                        else:\n",
    "                            subject_list = [subject.strip()]\n",
    "                    \n",
    "                    # Store in the map\n",
    "                    metadata_map[base_name] = {\n",
    "                        'title': title,\n",
    "                        'subjects': subject_list\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {i}: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded metadata for {len(metadata_map)} documents\")\n",
    "    \n",
    "    # Print a sample of the metadata for verification\n",
    "    print(\"\\nSample metadata entries:\")\n",
    "    sample_keys = list(metadata_map.keys())[:5]  # First 5 keys\n",
    "    for key in sample_keys:\n",
    "        print(f\"  {key}: {metadata_map[key]}\")\n",
    "    \n",
    "    return metadata_map\n",
    "\n",
    "def update_json_with_metadata(json_file_path, metadata_map):\n",
    "    \"\"\"\n",
    "    Update a consolidated JSON file with titles and subjects from the metadata map.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path (str): Path to the consolidated JSON file\n",
    "        metadata_map (dict): Dictionary mapping filenames to metadata\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of chunks updated\n",
    "    \"\"\"\n",
    "    print(f\"Updating file: {json_file_path}\")\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Count of updated chunks\n",
    "    updated_count = 0\n",
    "    total_chunks = len(data.get(\"chunks\", []))\n",
    "    \n",
    "    # Track which files were matched and which weren't\n",
    "    matched_files = set()\n",
    "    unmatched_files = set()\n",
    "    \n",
    "    # First pass: collect all unique source files\n",
    "    source_files = set()\n",
    "    source_file_to_chunks = {}\n",
    "    \n",
    "    for i, chunk in enumerate(data.get(\"chunks\", [])):\n",
    "        metadata = chunk.get(\"metadata\", {})\n",
    "        source_file = metadata.get(\"source_file\", \"\")\n",
    "        \n",
    "        if source_file:\n",
    "            source_files.add(source_file)\n",
    "            \n",
    "            # Group chunks by source file\n",
    "            if source_file not in source_file_to_chunks:\n",
    "                source_file_to_chunks[source_file] = []\n",
    "            source_file_to_chunks[source_file].append(i)\n",
    "    \n",
    "    print(f\"Found {len(source_files)} unique source files in {total_chunks} chunks\")\n",
    "    \n",
    "    # Print a sample of source files for debugging\n",
    "    print(\"Sample source files:\")\n",
    "    sample_source_files = list(source_files)[:5]\n",
    "    for source_file in sample_source_files:\n",
    "        print(f\"  - {source_file}\")\n",
    "    \n",
    "    # Second pass: match source files to metadata\n",
    "    source_file_metadata = {}\n",
    "    \n",
    "    for source_file in source_files:\n",
    "        # Extract base name without extension\n",
    "        base_name = os.path.splitext(source_file)[0]\n",
    "        \n",
    "        # Also try with .pdf extension\n",
    "        pdf_base_name = base_name + \".pdf\"\n",
    "        pdf_base_name_without_ext = os.path.splitext(pdf_base_name)[0]\n",
    "        \n",
    "        # Check for metadata match\n",
    "        metadata_entry = None\n",
    "        matched_key = None\n",
    "        \n",
    "        # Try multiple possible matching strategies\n",
    "        possible_keys = [\n",
    "            base_name,                                   # Direct match\n",
    "            pdf_base_name_without_ext,                   # Match with PDF extension\n",
    "            f\"PSC_{base_name}\" if not base_name.startswith(\"PSC_\") else None,  # With PSC_ prefix\n",
    "            base_name[4:] if base_name.startswith(\"PSC_\") else None,           # Without PSC_ prefix\n",
    "            base_name.replace(\"PSR_\", \"PSC_\") if base_name.startswith(\"PSR_\") else None,  # PSR to PSC\n",
    "            base_name.replace(\"PSC_\", \"PSR_\") if base_name.startswith(\"PSC_\") else None   # PSC to PSR\n",
    "        ]\n",
    "        \n",
    "        # Try all possible keys\n",
    "        for key in possible_keys:\n",
    "            if key and key in metadata_map:\n",
    "                metadata_entry = metadata_map[key]\n",
    "                matched_key = key\n",
    "                break\n",
    "        \n",
    "        # Try matching by date pattern if still not found\n",
    "        if not metadata_entry:\n",
    "            # Extract date pattern (YYYY_MM_DD or similar)\n",
    "            date_patterns = [\n",
    "                re.search(r'(\\d{4})[_-](\\d{2})[_-](\\d{2})', base_name),  # YYYY-MM-DD\n",
    "                re.search(r'(\\d{4})[_-](\\d{2})', base_name)               # YYYY-MM\n",
    "            ]\n",
    "            \n",
    "            for date_match in date_patterns:\n",
    "                if date_match:\n",
    "                    date_pattern = date_match.group(0)\n",
    "                    # Look for any entry with this date pattern\n",
    "                    for k, v in metadata_map.items():\n",
    "                        if date_pattern in k:\n",
    "                            metadata_entry = v\n",
    "                            matched_key = k\n",
    "                            break\n",
    "                    if metadata_entry:\n",
    "                        break\n",
    "        \n",
    "        # Special case for PSR format with hyphen\n",
    "        if not metadata_entry and 'PSR' in base_name:\n",
    "            # Extract year, month, day if present\n",
    "            psr_match = re.search(r'PSR (\\d{4})-(\\d{1,2})(?:-(\\d{1,2}))?', base_name)\n",
    "            if psr_match:\n",
    "                year = psr_match.group(1)\n",
    "                month = psr_match.group(2).zfill(2)  # Ensure 2 digits\n",
    "                day = psr_match.group(3).zfill(2) if psr_match.group(3) else None\n",
    "                \n",
    "                # Try to match with PSC_YYYY_MM_DD format\n",
    "                if day:\n",
    "                    psc_key = f\"PSC_{year}_{month}_{day}\"\n",
    "                else:\n",
    "                    psc_key = f\"PSC_{year}_{month}\"\n",
    "                \n",
    "                if psc_key in metadata_map:\n",
    "                    metadata_entry = metadata_map[psc_key]\n",
    "                    matched_key = psc_key\n",
    "        \n",
    "        # Store metadata if found\n",
    "        if metadata_entry:\n",
    "            source_file_metadata[source_file] = metadata_entry\n",
    "            matched_files.add(source_file)\n",
    "        else:\n",
    "            unmatched_files.add(source_file)\n",
    "    \n",
    "    # Third pass: update all chunks with matched metadata\n",
    "    chunks_list = data.get(\"chunks\", [])\n",
    "    \n",
    "    for source_file, metadata_entry in source_file_metadata.items():\n",
    "        # Get all chunk indices for this source file\n",
    "        chunk_indices = source_file_to_chunks.get(source_file, [])\n",
    "        \n",
    "        for idx in chunk_indices:\n",
    "            chunk = chunks_list[idx]\n",
    "            metadata = chunk.get(\"metadata\", {})\n",
    "            \n",
    "            # Only update if title is empty or not set\n",
    "            if not metadata.get(\"title\") or metadata.get(\"title\") == \"\":\n",
    "                metadata[\"title\"] = metadata_entry[\"title\"]\n",
    "            \n",
    "            # Only update subjects if empty\n",
    "            if not metadata.get(\"subjects\") or metadata.get(\"subjects\") == []:\n",
    "                metadata[\"subjects\"] = metadata_entry[\"subjects\"]\n",
    "            \n",
    "            updated_count += 1\n",
    "    \n",
    "    # Save the updated JSON back to the same file\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Updated {updated_count}/{total_chunks} chunks in {json_file_path}\")\n",
    "    print(f\"Matched {len(matched_files)} unique files, failed to match {len(unmatched_files)} files\")\n",
    "    \n",
    "    # Print some examples of unmatched files for debugging\n",
    "    if unmatched_files:\n",
    "        print(\"Examples of unmatched files:\")\n",
    "        for file in list(unmatched_files)[:5]:\n",
    "            print(f\"  - {file}\")\n",
    "    \n",
    "    return updated_count\n",
    "\n",
    "def update_all_years(consolidated_dir, metadata_files):\n",
    "    \"\"\"\n",
    "    Update all year JSON files with metadata.\n",
    "    \n",
    "    Args:\n",
    "        consolidated_dir (str): Directory containing consolidated JSON files\n",
    "        metadata_files (list): List of paths to metadata files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Summary of updates by year\n",
    "    \"\"\"\n",
    "    # Load all metadata\n",
    "    metadata_map = load_metadata_from_excel_csv(metadata_files)\n",
    "    \n",
    "    # Get all consolidated JSON files\n",
    "    json_pattern = os.path.join(consolidated_dir, \"psc_*_all_chunks.json\")\n",
    "    json_files = glob.glob(json_pattern)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        # Extract year from filename\n",
    "        year_match = re.search(r'psc_(\\d{4})_all_chunks', os.path.basename(json_file))\n",
    "        if year_match:\n",
    "            year = year_match.group(1)\n",
    "            \n",
    "            # Update the file\n",
    "            updated_count = update_json_with_metadata(json_file, metadata_map)\n",
    "            \n",
    "            results[year] = {\n",
    "                \"file\": os.path.basename(json_file),\n",
    "                \"updated_chunks\": updated_count\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Cell 6: Explore the metadata files\n",
    "def explore_metadata_file(file_path):\n",
    "    \"\"\"View a sample of the metadata file to confirm structure\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "            try:\n",
    "                df = pd.read_excel(file_path)\n",
    "            except ImportError as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(\"Trying to read CSV instead...\")\n",
    "                # Check if CSV version exists\n",
    "                csv_version = file_path.rsplit('.', 1)[0] + '.csv'\n",
    "                if os.path.exists(csv_version):\n",
    "                    df = pd.read_csv(csv_version)\n",
    "                else:\n",
    "                    print(f\"CSV version not found: {csv_version}\")\n",
    "                    return None\n",
    "        elif file_path.endswith('.csv'):\n",
    "            # Detect encoding\n",
    "            encoding = detect_encoding(file_path)\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"Total rows: {len(df)}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(\"\\nSample data (first 5 rows):\")\n",
    "        return df.head()\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run this to explore a metadata file\n",
    "# Uncomment to explore your files\n",
    "# if METADATA_FILES:\n",
    "#     explore_metadata_file(METADATA_FILES[0])\n",
    "\n",
    "# Cell 7: Update a specific year\n",
    "# For updating a single year (adjust the year as needed)\n",
    "def update_specific_year(year, metadata_files, consolidated_dir):\n",
    "    metadata_map = load_metadata_from_excel_csv(metadata_files)\n",
    "    json_file_path = os.path.join(consolidated_dir, f\"psc_{year}_all_chunks.json\")\n",
    "    \n",
    "    if os.path.exists(json_file_path):\n",
    "        updated_count = update_json_with_metadata(json_file_path, metadata_map)\n",
    "        print(f\"\\nUpdated {updated_count} chunks in the {year} file\")\n",
    "    else:\n",
    "        print(f\"\\nFile not found: {json_file_path}\")\n",
    "\n",
    "# Cell 8: Main execution - run this to process a specific year\n",
    "# Replace with the year you want to process\n",
    "# YEAR_TO_PROCESS = \"1974\"\n",
    "\n",
    "# print(f\"Processing year: {YEAR_TO_PROCESS}\")\n",
    "# update_specific_year(YEAR_TO_PROCESS, METADATA_FILES, CONSOLIDATED_DIR)\n",
    "\n",
    "# Cell 9: Update all years at once\n",
    "# Run this to update all years\n",
    "def update_all_years_with_summary():\n",
    "    results = update_all_years(CONSOLIDATED_DIR, METADATA_FILES)\n",
    "    \n",
    "    print(\"\\nUpdate results by year:\")\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    summary_data = []\n",
    "    for year, result in sorted(results.items()):\n",
    "        summary_data.append({\n",
    "            \"Year\": year,\n",
    "            \"File\": result['file'],\n",
    "            \"Updated Chunks\": result['updated_chunks']\n",
    "        })\n",
    "    \n",
    "    # Try to create a DataFrame, but handle the case where pandas might be limited\n",
    "    try:\n",
    "        return pd.DataFrame(summary_data)\n",
    "    except:\n",
    "        # Print in tabular format if DataFrame creation fails\n",
    "        print(\"\\nYear | File | Updated Chunks\")\n",
    "        print(\"-\" * 40)\n",
    "        for item in summary_data:\n",
    "            print(f\"{item['Year']} | {item['File']} | {item['Updated Chunks']}\")\n",
    "        return summary_data\n",
    "\n",
    "# Uncomment this line to update all years\n",
    "update_all_years_with_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
