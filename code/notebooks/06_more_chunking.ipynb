{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41255e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Config ---\n",
    "PDF_PATH = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/raw/A Choice Not An Echo 2014 7-23-14.pdf\"\n",
    "OUTPUT_JSON = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/choice_not_echo_2014.json\"\n",
    "\n",
    "BOOK_METADATA = {\n",
    "    \"author\": \"Phyllis Schlafly\",\n",
    "    \"book_title\": \"A Choice Not An Echo, 2014 Edition\",\n",
    "    \"publication_year\": 2014\n",
    "}\n",
    "\n",
    "# --- Step 1: Load PDF using LangChain ---\n",
    "def load_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# --- Step 2: Split into chunks using LangChain text splitter ---\n",
    "def chunk_documents(docs, chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# --- Step 3: Structure chunks with metadata ---\n",
    "def structure_chunks(chunks, metadata):\n",
    "    return [\n",
    "        {\n",
    "            **metadata,\n",
    "            \"text\": chunk.page_content.strip()\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "# --- Step 4: Save as JSON ---\n",
    "def write_json(data, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- Full Pipeline ---\n",
    "def process_pdf():\n",
    "    print(\"Loading PDF...\")\n",
    "    docs = load_pdf(PDF_PATH)\n",
    "\n",
    "    print(\"Chunking...\")\n",
    "    chunks = chunk_documents(docs)\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "    print(\"Structuring output...\")\n",
    "    structured = structure_chunks(chunks, BOOK_METADATA)\n",
    "\n",
    "    print(\"Writing JSON...\")\n",
    "    write_json(structured, OUTPUT_JSON)\n",
    "    print(f\"Done! Output saved to: {OUTPUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522d7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Chunking...\n",
      "Total chunks: 547\n",
      "Structuring output...\n",
      "Writing JSON...\n",
      "Done! Output saved to: /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/choice_not_echo_2014.json\n"
     ]
    }
   ],
   "source": [
    "process_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20467dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "# --- Config ---\n",
    "PDF_PATH = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/raw/ALL001.pdf\"\n",
    "OUTPUT_JSON = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/allegiance.json\"\n",
    "\n",
    "BOOK_METADATA = {\n",
    "    \"author\": \"Phyllis Schlafly\",\n",
    "    \"book_title\": \"Allegiance: Briefing Book on American Independence & Sovereignty\",\n",
    "    \"publication_year\": 2000\n",
    "}\n",
    "\n",
    "# --- Step 1: OCR PDF pages ---\n",
    "def ocr_pdf(pdf_path):\n",
    "    print(\"Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path)\n",
    "\n",
    "    print(\"Running OCR on each page...\")\n",
    "    full_text = \"\"\n",
    "    for i, image in enumerate(images):\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        full_text += text + \"\\n\"\n",
    "        print(f\"OCR done for page {i+1}/{len(images)}\")\n",
    "\n",
    "    return full_text\n",
    "\n",
    "# --- Step 2: Chunk the text ---\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.create_documents([text])\n",
    "\n",
    "# --- Step 3: Structure chunks with metadata ---\n",
    "def structure_chunks(chunks, metadata):\n",
    "    return [\n",
    "        {\n",
    "            **metadata,\n",
    "            \"text\": chunk.page_content.strip()\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "# --- Step 4: Save as JSON ---\n",
    "def write_json(data, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- Run full process ---\n",
    "def process_non_ocr_pdf():\n",
    "    print(\"Starting OCR + Chunking Pipeline...\")\n",
    "    text = ocr_pdf(PDF_PATH)\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(text)\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "    print(\"Structuring output...\")\n",
    "    structured = structure_chunks(chunks, BOOK_METADATA)\n",
    "\n",
    "    print(\"Writing JSON...\")\n",
    "    write_json(structured, OUTPUT_JSON)\n",
    "    print(f\"Done! Output saved to: {OUTPUT_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bd29e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OCR + Chunking Pipeline...\n",
      "Converting PDF pages to images...\n",
      "Running OCR on each page...\n",
      "OCR done for page 1/50\n",
      "OCR done for page 2/50\n",
      "OCR done for page 3/50\n",
      "OCR done for page 4/50\n",
      "OCR done for page 5/50\n",
      "OCR done for page 6/50\n",
      "OCR done for page 7/50\n",
      "OCR done for page 8/50\n",
      "OCR done for page 9/50\n",
      "OCR done for page 10/50\n",
      "OCR done for page 11/50\n",
      "OCR done for page 12/50\n",
      "OCR done for page 13/50\n",
      "OCR done for page 14/50\n",
      "OCR done for page 15/50\n",
      "OCR done for page 16/50\n",
      "OCR done for page 17/50\n",
      "OCR done for page 18/50\n",
      "OCR done for page 19/50\n",
      "OCR done for page 20/50\n",
      "OCR done for page 21/50\n",
      "OCR done for page 22/50\n",
      "OCR done for page 23/50\n",
      "OCR done for page 24/50\n",
      "OCR done for page 25/50\n",
      "OCR done for page 26/50\n",
      "OCR done for page 27/50\n",
      "OCR done for page 28/50\n",
      "OCR done for page 29/50\n",
      "OCR done for page 30/50\n",
      "OCR done for page 31/50\n",
      "OCR done for page 32/50\n",
      "OCR done for page 33/50\n",
      "OCR done for page 34/50\n",
      "OCR done for page 35/50\n",
      "OCR done for page 36/50\n",
      "OCR done for page 37/50\n",
      "OCR done for page 38/50\n",
      "OCR done for page 39/50\n",
      "OCR done for page 40/50\n",
      "OCR done for page 41/50\n",
      "OCR done for page 42/50\n",
      "OCR done for page 43/50\n",
      "OCR done for page 44/50\n",
      "OCR done for page 45/50\n",
      "OCR done for page 46/50\n",
      "OCR done for page 47/50\n",
      "OCR done for page 48/50\n",
      "OCR done for page 49/50\n",
      "OCR done for page 50/50\n",
      "Chunking text...\n",
      "Total chunks: 136\n",
      "Structuring output...\n",
      "Writing JSON...\n",
      "Done! Output saved to: /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/allegiance.json\n"
     ]
    }
   ],
   "source": [
    "process_non_ocr_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee36d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Config ---\n",
    "PDF_PATH = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/raw/Schlafly_Phy_4FNL.pdf\"\n",
    "OUTPUT_JSON = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/interview.json\"\n",
    "\n",
    "# --- LangChain Loader ---\n",
    "def load_pdf_text(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# --- Strip copyright and reader note section ---\n",
    "def remove_front_matter(text):\n",
    "    start_marker = \"COPYRIGHT\"\n",
    "    end_marker = \"reviewed by the interviewee\"\n",
    "\n",
    "    start = text.find(start_marker)\n",
    "    end = text.find(end_marker)\n",
    "    if start != -1 and end != -1:\n",
    "        return text[end + len(end_marker):].strip()\n",
    "    return text\n",
    "\n",
    "# --- Extract interviews from text ---\n",
    "def extract_interviews(text):\n",
    "    pattern = re.compile(r\"(Interview #\\s*\\d+:\\s*[\\w\\s,]+\\d{4})\", re.IGNORECASE)\n",
    "    matches = list(pattern.finditer(text))\n",
    "\n",
    "    interviews = []\n",
    "    for i in range(len(matches)):\n",
    "        start = matches[i].start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "        header = matches[i].group(1)\n",
    "\n",
    "        number_match = re.search(r\"Interview #\\s*(\\d+)\", header)\n",
    "        date_match = re.search(r\":\\s*([\\w\\s,]+\\d{4})\", header)\n",
    "\n",
    "        interview_number = int(number_match.group(1)) if number_match else None\n",
    "        interview_date = date_match.group(1).strip() if date_match else None\n",
    "\n",
    "        segment_text = text[start:end].strip()\n",
    "\n",
    "        interviews.append({\n",
    "            \"interview_number\": interview_number,\n",
    "            \"interview_date\": interview_date,\n",
    "            \"text\": segment_text\n",
    "        })\n",
    "\n",
    "    return interviews\n",
    "\n",
    "# --- Chunk each interview ---\n",
    "def chunk_and_structure(interviews):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    final_chunks = []\n",
    "\n",
    "    for interview in interviews:\n",
    "        chunks = splitter.split_text(interview[\"text\"])\n",
    "        for chunk in chunks:\n",
    "            final_chunks.append({\n",
    "                \"author\": \"Phyllis Schlafly\",\n",
    "                \"interview_number\": interview[\"interview_number\"],\n",
    "                \"interview_date\": interview[\"interview_date\"],\n",
    "                \"interviewer\": \"Mark DePue\",\n",
    "                \"source_type\": \"Interview Transcript\",\n",
    "                \"text\": chunk.strip()\n",
    "            })\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "# --- Write JSON ---\n",
    "def write_json(data, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- Main pipeline ---\n",
    "def process_interview_pdf():\n",
    "    print(\"Loading PDF...\")\n",
    "    full_text = load_pdf_text(PDF_PATH)\n",
    "\n",
    "    print(\"Removing front matter...\")\n",
    "    cleaned_text = remove_front_matter(full_text)\n",
    "\n",
    "    print(\"Extracting interviews...\")\n",
    "    interviews = extract_interviews(cleaned_text)\n",
    "    print(f\"Found {len(interviews)} interviews.\")\n",
    "\n",
    "    print(\"Chunking...\")\n",
    "    structured_chunks = chunk_and_structure(interviews)\n",
    "\n",
    "    print(\"Saving to JSON...\")\n",
    "    write_json(structured_chunks, OUTPUT_JSON)\n",
    "    print(f\"Done! Saved to {OUTPUT_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e642dc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Removing front matter...\n",
      "Extracting interviews...\n",
      "Found 6 interviews.\n",
      "Chunking...\n",
      "Saving to JSON...\n",
      "Done! Saved to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/interview.json\n"
     ]
    }
   ],
   "source": [
    "process_interview_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae299a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Removing front matter...\n",
      "Extracting Interview #1...\n",
      "Chunking Interview #1...\n",
      "Appending 80 chunks to JSON...\n",
      "✅ Done! Interview #1 appended to: /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/interview.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Config ---\n",
    "PDF_PATH = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/raw/Schlafly_Phy_4FNL.pdf\"\n",
    "OUTPUT_JSON = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/interview.json\"\n",
    "\n",
    "# --- Load full text using LangChain ---\n",
    "def load_pdf_text(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# --- Remove boilerplate front matter ---\n",
    "def remove_front_matter(text):\n",
    "    start_marker = \"COPYRIGHT\"\n",
    "    end_marker = \"reviewed by the interviewee\"\n",
    "\n",
    "    start = text.find(start_marker)\n",
    "    end = text.find(end_marker)\n",
    "    if start != -1 and end != -1:\n",
    "        return text[end + len(end_marker):].strip()\n",
    "    return text\n",
    "\n",
    "# --- Extract text from start until \"Interview # 2\" ---\n",
    "def extract_interview_one(text):\n",
    "    match = re.search(r\"Interview #\\s*2\\s*[:\\-]\", text, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        return text[:match.start()].strip()\n",
    "    return text  # fallback: return full text if marker not found\n",
    "\n",
    "# --- Chunk and structure Interview #1 ---\n",
    "def chunk_interview_one(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"author\": \"Phyllis Schlafly\",\n",
    "            \"interview_number\": 1,\n",
    "            \"interview_date\": \"January 5, 2011\",\n",
    "            \"interviewer\": \"Mark DePue\",\n",
    "            \"source_type\": \"Interview Transcript\",\n",
    "            \"text\": chunk.strip()\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "# --- Append to existing JSON ---\n",
    "def append_to_json(new_data, json_path):\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    combined = existing_data + new_data\n",
    "\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- Run the pipeline ---\n",
    "def process_interview_one():\n",
    "    print(\"Loading PDF...\")\n",
    "    full_text = load_pdf_text(PDF_PATH)\n",
    "\n",
    "    print(\"Removing front matter...\")\n",
    "    cleaned_text = remove_front_matter(full_text)\n",
    "\n",
    "    print(\"Extracting Interview #1...\")\n",
    "    interview_one_text = extract_interview_one(cleaned_text)\n",
    "\n",
    "    print(\"Chunking Interview #1...\")\n",
    "    chunks = chunk_interview_one(interview_one_text)\n",
    "\n",
    "    print(f\"Appending {len(chunks)} chunks to JSON...\")\n",
    "    append_to_json(chunks, OUTPUT_JSON)\n",
    "\n",
    "    print(f\"✅ Done! Interview #1 appended to: {OUTPUT_JSON}\")\n",
    "\n",
    "process_interview_one()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
