{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ec6d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Path Setup\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from difflib import get_close_matches\n",
    "\n",
    "RAW_DATA_DIR = Path(\"D:/Technical_projects/PSAI/raw_data/PSR\")\n",
    "INDEX_JSON = Path(\"D:/Technical_projects/PSAI/code/psrindex.json\")\n",
    "CHUNKS_DIR = Path(\"D:/Technical_projects/PSAI/chunks\")\n",
    "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90564eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Normalize Index\n",
    "\n",
    "def normalize_index(index_path):\n",
    "    with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "        index_data = raw[\"report_index\"] if isinstance(raw, dict) and \"report_index\" in raw else raw\n",
    "\n",
    "    normalized = {}\n",
    "    for entry in index_data:\n",
    "        month, day, year = entry[\"date\"].split(\"/\")\n",
    "        key = f\"{year.zfill(4)}-{month.zfill(2)}\"\n",
    "        normalized[key] = {\n",
    "            \"articles\": entry[\"articles\"],\n",
    "            \"subjects\": entry[\"subjects\"]\n",
    "        }\n",
    "    return normalized\n",
    "\n",
    "index_lookup = normalize_index(INDEX_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42b668f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Extract YYYY-MM from Filename\n",
    "\n",
    "def extract_date_key_from_filename(filename):\n",
    "    match = re.search(r\"(\\d{6})\\.pdf\", filename)\n",
    "    if match:\n",
    "        yyyymm = match.group(1)\n",
    "        return f\"{yyyymm[:4]}-{yyyymm[4:]}\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9905f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Split Text by Article Titles\n",
    "\n",
    "def split_by_titles(text, titles):\n",
    "    article_chunks = []\n",
    "    current_title = None\n",
    "    current_text = \"\"\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        match = get_close_matches(stripped, titles, n=1, cutoff=0.85)\n",
    "\n",
    "        if match:\n",
    "            # Save current chunk\n",
    "            if current_text:\n",
    "                article_chunks.append((current_title or \"Untitled\", current_text.strip()))\n",
    "            # Start new chunk\n",
    "            current_title = match[0]\n",
    "            current_text = stripped + \"\\n\"\n",
    "        else:\n",
    "            current_text += stripped + \"\\n\"\n",
    "\n",
    "    if current_text:\n",
    "        article_chunks.append((current_title or \"Untitled\", current_text.strip()))\n",
    "\n",
    "    return article_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60a4b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Main Chunking Function\n",
    "\n",
    "def process_pdf_with_index(pdf_path, index):\n",
    "    date_key = extract_date_key_from_filename(pdf_path.name)\n",
    "    if date_key not in index:\n",
    "        print(f\"Skipping {pdf_path.name}: No index entry for date {date_key}\")\n",
    "        return []\n",
    "\n",
    "    entry = index[date_key]\n",
    "    articles = entry[\"articles\"]\n",
    "    subjects = entry[\"subjects\"]\n",
    "    year, month = date_key.split(\"-\")\n",
    "    month_names = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "                   \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "    readable_date = f\"{month_names[int(month)-1]}, {year}\"\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    page_map = []\n",
    "\n",
    "    for page_number, page in enumerate(doc, start=1):\n",
    "        page_text = page.get_text()\n",
    "        page_map.append((page_number, len(full_text)))\n",
    "        full_text += page_text + \"\\n\"\n",
    "\n",
    "    chunks = []\n",
    "    article_chunks = split_by_titles(full_text, articles)\n",
    "\n",
    "    for title, text in article_chunks:\n",
    "        # Estimate page number from character offset\n",
    "        first_char_offset = full_text.find(text[:30])\n",
    "        page_number = next((pn for pn, offset in reversed(page_map) if first_char_offset >= offset), 1)\n",
    "\n",
    "        chunks.append({\n",
    "            \"text\": text.strip(),\n",
    "            \"metadata\": {\n",
    "                \"title\": title,\n",
    "                \"date\": readable_date,\n",
    "                \"author\": \"Phyllis Schlafly\",\n",
    "                \"subjects\": subjects,\n",
    "                \"page_number\": page_number,\n",
    "                \"source_file\": pdf_path.name,\n",
    "                \"doc_type\": \"Phyllis Schlafly Report\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3d37552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Process All PDFs and Save Output\n",
    "\n",
    "def chunk_all_psrs():\n",
    "    all_chunks = []\n",
    "    pdf_files = sorted(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "    for pdf in tqdm(pdf_files):\n",
    "        if \"(1)\" in pdf.stem:\n",
    "            print(f\"Skipping duplicate file: {pdf.name}\")\n",
    "            continue\n",
    "        chunks = process_pdf_with_index(pdf, index_lookup)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    output_path = CHUNKS_DIR / \"all_chunks.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… Saved {len(all_chunks)} chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54bbf913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 171/593 [00:06<00:12, 34.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_14_06_198101(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_14_07_198102(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_14_08_198103(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_14_09_198104(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_14_10_198105(1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–‰       | 177/593 [00:07<00:11, 37.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_14_11_198106(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_14_12_198107(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_15_01_198108(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_15_02_198109(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_15_03_198110(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_15_04_198111(1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 188/593 [00:07<00:10, 40.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_15_05_198112(1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 373/593 [00:13<00:05, 38.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_31_06_199801(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_31_07_199802(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_31_08_199803(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_31_09_199804(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_31_10_199805(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_31_11_199806(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_31_12_199807(1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 385/593 [00:14<00:05, 41.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_32_01_199808(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_32_02_199809(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_32_03_199810(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_32_04_199811(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_32_05_199812(1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 497/593 [00:17<00:02, 40.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_41_06_200801(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_41_07_200802(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_41_08_200803(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_41_09_200804(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_41_10_200805(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_41_11_200806(1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 509/593 [00:18<00:01, 47.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_41_12_200807(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_42_01_200808(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_42_02_200809(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_42_03_200810(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_42_04_200811(1).pdf\n",
      "Skipping duplicate file: PSCA_PSR_42_05_200812(1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 569/593 [00:19<00:00, 49.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_47_06_201401(1).pdf\n",
      "Skipping PSCA_PSR_47_06_201401(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_47_07_201402(1).pdf\n",
      "Skipping PSCA_PSR_47_07_201402(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_47_08_201403(1).pdf\n",
      "Skipping PSCA_PSR_47_08_201403(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_47_09_201404(1).pdf\n",
      "Skipping PSCA_PSR_47_09_201404(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_47_10_201405(1).pdf\n",
      "Skipping PSCA_PSR_47_10_201405(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_47_11_201406(1).pdf\n",
      "Skipping PSCA_PSR_47_11_201406(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_47_12_201407(1).pdf\n",
      "Skipping PSCA_PSR_47_12_201407(2).pdf: No index entry for date None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 593/593 [00:20<00:00, 65.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate file: PSCA_PSR_48_01_201408(1).pdf\n",
      "Skipping PSCA_PSR_48_01_201408(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_48_02_201409(1).pdf\n",
      "Skipping PSCA_PSR_48_02_201409(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_48_03_201410(1).pdf\n",
      "Skipping PSCA_PSR_48_03_201410(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_48_04_201411(1).pdf\n",
      "Skipping PSCA_PSR_48_04_201411(2).pdf: No index entry for date None\n",
      "Skipping duplicate file: PSCA_PSR_48_05_201412(1).pdf\n",
      "Skipping PSCA_PSR_48_05_201412(2).pdf: No index entry for date None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 593/593 [00:20<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 1232 chunks to D:\\Technical_projects\\PSAI\\chunks\\all_chunks.json\n"
     ]
    }
   ],
   "source": [
    "chunk_all_psrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a517015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def summarize_titles(all_chunks_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Extracts just the date and title for each chunk from all_chunks.json.\n",
    "    Optionally saves to a .jsonl file or prints sample output.\n",
    "    \"\"\"\n",
    "    all_chunks_path = Path(all_chunks_path)\n",
    "    \n",
    "    with open(all_chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks = json.load(f)\n",
    "\n",
    "    summary = [\n",
    "        {\n",
    "            \"date\": chunk[\"metadata\"][\"date\"],\n",
    "            \"title\": chunk[\"metadata\"][\"title\"],\n",
    "            \"source_file\": chunk[\"metadata\"][\"source_file\"]\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "    if output_path:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "            json.dump(summary, out, indent=2, ensure_ascii=False)\n",
    "        print(f\"âœ… Summary saved to {output_path}\")\n",
    "    else:\n",
    "        # Just print a sample\n",
    "        for entry in summary[:10]:\n",
    "            print(entry)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78f8b39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summary saved to D:/Technical_projects/PSAI/chunks/title_summary.json\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_titles(\n",
    "    \"D:/Technical_projects/PSAI/chunks/all_chunks.json\",\n",
    "    output_path=\"D:/Technical_projects/PSAI/chunks/title_summary.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e03d5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_title_with_all_titles(chunks_path, index_path, output_path):\n",
    "    # Load chunks\n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks = json.load(f)\n",
    "\n",
    "    # Load and normalize the index\n",
    "    with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_index = json.load(f)[\"report_index\"]\n",
    "\n",
    "    index_lookup = {}\n",
    "    for entry in raw_index:\n",
    "        m, d, y = entry[\"date\"].split(\"/\")\n",
    "        key = f\"{y.zfill(4)}-{m.zfill(2)}\"\n",
    "        index_lookup[key] = entry[\"articles\"]\n",
    "\n",
    "    # Update each chunk\n",
    "    updated = 0\n",
    "    for chunk in chunks:\n",
    "        filename = chunk[\"metadata\"][\"source_file\"]\n",
    "        match = re.search(r\"(\\d{6})\\.pdf\", filename)\n",
    "        if match:\n",
    "            yyyymm = match.group(1)\n",
    "            key = f\"{yyyymm[:4]}-{yyyymm[4:]}\"\n",
    "            if key in index_lookup:\n",
    "                chunk[\"metadata\"][\"title\"] = index_lookup[key]\n",
    "                updated += 1\n",
    "\n",
    "    # Save the updated version\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Updated {updated} chunks with full article title lists.\")\n",
    "    print(f\"ðŸ“„ Output saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "128458e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated 1232 chunks with full article title lists.\n",
      "ðŸ“„ Output saved to: D:/Technical_projects/PSAI/chunks/all_chunks_titlelists.json\n"
     ]
    }
   ],
   "source": [
    "replace_title_with_all_titles(\n",
    "    chunks_path=\"D:/Technical_projects/PSAI/chunks/all_chunks.json\",\n",
    "    index_path=\"D:/Technical_projects/PSAI/code/psrindex.json\",\n",
    "    output_path=\"D:/Technical_projects/PSAI/chunks/all_chunks_titlelists.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fd99371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_chunks(chunks, max_len=1000):\n",
    "    new_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text = chunk[\"text\"]\n",
    "        metadata = chunk[\"metadata\"]\n",
    "\n",
    "        if len(text) <= max_len:\n",
    "            new_chunks.append(chunk)\n",
    "        else:\n",
    "            # Split by paragraph breaks first\n",
    "            parts = text.split(\"\\n\\n\")\n",
    "            buffer = \"\"\n",
    "\n",
    "            for part in parts:\n",
    "                if len(buffer) + len(part) < max_len:\n",
    "                    buffer += part.strip() + \"\\n\\n\"\n",
    "                else:\n",
    "                    if buffer.strip():\n",
    "                        new_chunks.append({\n",
    "                            \"text\": buffer.strip(),\n",
    "                            \"metadata\": metadata.copy()\n",
    "                        })\n",
    "                    buffer = part.strip() + \"\\n\\n\"\n",
    "\n",
    "            if buffer.strip():\n",
    "                new_chunks.append({\n",
    "                    \"text\": buffer.strip(),\n",
    "                    \"metadata\": metadata.copy()\n",
    "                })\n",
    "\n",
    "    print(f\"âœ… Split into {len(new_chunks)} chunks (was {len(chunks)})\")\n",
    "    return new_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03c6e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Split into 4621 chunks (was 1232)\n",
      "ðŸ“„ Final chunk file written.\n"
     ]
    }
   ],
   "source": [
    "# Load the previously updated chunk file\n",
    "with open(\"D:/Technical_projects/PSAI/chunks/all_chunks_titlelists.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    original_chunks = json.load(f)\n",
    "\n",
    "# Split them\n",
    "smaller_chunks = split_large_chunks(original_chunks, max_len=1000)\n",
    "\n",
    "# Save to new file\n",
    "with open(\"D:/Technical_projects/PSAI/chunks/all_chunks_final.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(smaller_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ðŸ“„ Final chunk file written.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
