{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f370f5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mason/opt/anaconda3/envs/psai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de85354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to Qdrant at https://3031677a-6463-44f9-ba66-42977581720e.us-east-1-0.aws.cloud.qdrant.io\n",
      "‚úÖ Loaded embedding model: all-MiniLM-L6-v2 (output dim: 384)\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "try:\n",
    "    load_dotenv(\"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/code/.env\")\n",
    "    \n",
    "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "    \n",
    "    if not QDRANT_URL or not QDRANT_API_KEY:\n",
    "        raise ValueError(\"QDRANT_URL or QDRANT_API_KEY environment variables not found\")\n",
    "    \n",
    "    # Initialize Qdrant client and embedding model\n",
    "    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "    print(f\"‚úÖ Successfully connected to Qdrant at {QDRANT_URL}\")\n",
    "    \n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    print(f\"‚úÖ Loaded embedding model: all-MiniLM-L6-v2 (output dim: {model.get_sentence_embedding_dimension()})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during initialization: {e}\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311a4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_collection(name):\n",
    "    \"\"\"Create collection if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        collections = [c.name for c in client.get_collections().collections]\n",
    "        if name not in collections:\n",
    "            client.recreate_collection(\n",
    "                collection_name=name,\n",
    "                vectors_config=VectorParams(size=model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "            print(f\"‚úÖ Created collection: {name}\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Collection {name} already exists\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error ensuring collection {name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def embed_and_upload(chunks, collection_name):\n",
    "    \"\"\"Embed text chunks and upload them to Qdrant.\"\"\"\n",
    "    if not chunks:\n",
    "        print(f\"‚ö†Ô∏è Warning: No chunks to upload to {collection_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üì¶ Processing {len(chunks)} chunks for collection {collection_name}...\")\n",
    "    ensure_collection(collection_name)\n",
    "    \n",
    "    batch_size = 100\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        texts = [chunk[\"text\"] for chunk in batch]\n",
    "        \n",
    "        try:\n",
    "            embeddings = model.encode(texts, show_progress_bar=True).tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error embedding batch starting at index {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        points = []\n",
    "        for chunk, embedding in zip(batch, embeddings):\n",
    "            payload = chunk.get(\"metadata\", {}).copy()\n",
    "            payload[\"text\"] = chunk[\"text\"]\n",
    "            points.append(\n",
    "                PointStruct(\n",
    "                    id=str(uuid4()),\n",
    "                    vector=embedding,\n",
    "                    payload=payload\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            client.upload_points(collection_name=collection_name, points=points)\n",
    "            print(f\"‚úÖ Uploaded batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1} to {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error uploading batch to {collection_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0114fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_commentaries(commentaries_path, net_tv_path):\n",
    "    \"\"\"Load commentary and NET-TV chunks into a single list.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Load Commentaries (normal)\n",
    "    with open(commentaries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        commentary_data = json.load(f)\n",
    "        for entry in commentary_data:\n",
    "            chunks.append({\n",
    "                \"text\": entry[\"text\"],\n",
    "                \"metadata\": entry[\"metadata\"]\n",
    "            })\n",
    "    \n",
    "    # Load NET-TV (full_text remapping)\n",
    "    with open(net_tv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        net_tv_data = json.load(f)\n",
    "        for entry in net_tv_data:\n",
    "            chunks.append({\n",
    "                \"text\": entry[\"full_text\"],\n",
    "                \"metadata\": {\n",
    "                    \"author\": entry.get(\"metadata\", {}).get(\"author\", \"Unknown\"),\n",
    "                    \"title\": entry.get(\"metadata\", {}).get(\"title\", \"Unknown\"),\n",
    "                    \"publication\": entry.get(\"metadata\", {}).get(\"publication\", \"NET-TV\"),\n",
    "                    \"date_recorded\": entry.get(\"metadata\", {}).get(\"date_recorded\", \"Unknown\")\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(chunks)} commentary + NET-TV chunks\")\n",
    "    return chunks\n",
    "\n",
    "def load_books(book_paths):\n",
    "    \"\"\"Load book chunks from multiple JSON files.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for book_path in book_paths:\n",
    "        with open(book_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            book_data = json.load(f)\n",
    "            for entry in book_data:\n",
    "                chunks.append({\n",
    "                    \"text\": entry[\"text\"],\n",
    "                    \"metadata\": {\n",
    "                        \"author\": entry.get(\"author\", \"Unknown\"),\n",
    "                        \"book_title\": entry.get(\"book_title\", \"Unknown\"),\n",
    "                        \"publication_year\": entry.get(\"publication_year\", \"Unknown\"),\n",
    "                        \"doc_type\": \"Phyllis Schlafly Book\",\n",
    "                        \"source_file\": Path(book_path).name\n",
    "                    }\n",
    "                })\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(chunks)} book chunks\")\n",
    "    return chunks\n",
    "\n",
    "def load_columns(columns_path):\n",
    "    \"\"\"Load othercolumns.json into chunks.\"\"\"\n",
    "    with open(columns_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        columns_data = json.load(f)\n",
    "    \n",
    "    chunks = []\n",
    "    for entry in columns_data:\n",
    "        chunks.append({\n",
    "            \"text\": entry[\"text\"],\n",
    "            \"metadata\": entry[\"metadata\"]\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(chunks)} columns chunks\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e9fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 323 commentary + NET-TV chunks\n",
      "‚úÖ Loaded 866 book chunks\n",
      "‚úÖ Loaded 20 columns chunks\n",
      "üì¶ Processing 323 chunks for collection commentaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/s2pyk63x5958wcxdtr6xlpwc0000gn/T/ipykernel_36363/1754979787.py:6: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created collection: commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 1/4 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 2/4 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 3/4 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 4/4 to commentaries\n",
      "üì¶ Processing 866 chunks for collection book_chunks...\n",
      "‚ÑπÔ∏è Collection book_chunks already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 1/9 to book_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 2/9 to book_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 3/9 to book_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 4/9 to book_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 5/9 to book_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 6/9 to book_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 7/9 to book_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 8/9 to book_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 9/9 to book_chunks\n",
      "üì¶ Processing 20 chunks for collection columns_chunks...\n",
      "‚úÖ Created collection: columns_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 1/1 to columns_chunks\n"
     ]
    }
   ],
   "source": [
    "# Define your file paths\n",
    "commentaries_path = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/batch2/commentaries/2002.json\"\n",
    "net_tv_path = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/batch2/NET-TV.json\"\n",
    "\n",
    "book_paths = [\n",
    "    \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/batch2/allegiance.json\",\n",
    "    \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/batch2/choice_not_echo_2014.json\",\n",
    "    \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/batch2/how_mass_immigration.json\"\n",
    "]\n",
    "\n",
    "columns_path = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/batch2/othercolumns.json\"\n",
    "\n",
    "# Load the chunks\n",
    "commentary_chunks = load_commentaries(commentaries_path, net_tv_path)\n",
    "book_chunks = load_books(book_paths)\n",
    "columns_chunks = load_columns(columns_path)\n",
    "\n",
    "# Upload\n",
    "embed_and_upload(commentary_chunks, \"commentaries\")\n",
    "embed_and_upload(book_chunks, \"book_chunks\")\n",
    "embed_and_upload(columns_chunks, \"columns_chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37899a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 22 remaining commentary JSON files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading remaining commentaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:00<00:00, 112.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 5700 remaining commentary chunks\n",
      "üì¶ Processing 5700 chunks for collection commentaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Collection commentaries already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 1/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 2/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 3/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 4/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 5/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 6/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 7/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 8/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 9/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 10/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 11/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 12/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 13/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 14/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 15/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 16/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 17/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 18/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 19/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 20/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 21/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 22/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 23/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 24/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 25/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 26/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 27/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 28/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 29/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 30/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 31/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 32/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 33/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 34/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 35/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 36/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 37/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 38/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 39/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 40/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 41/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 42/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 43/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 44/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 45/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 46/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 47/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 48/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 49/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 50/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 51/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 52/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 53/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 54/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 55/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 56/57 to commentaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 57/57 to commentaries\n"
     ]
    }
   ],
   "source": [
    "def load_remaining_commentaries(commentaries_dir, skip_files=[\"2002.json\"]):\n",
    "    \"\"\"Load only the remaining commentary JSONs (2003‚Äì2024).\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    commentaries_dir = Path(commentaries_dir)\n",
    "    if not commentaries_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è Commentary folder {commentaries_dir} does not exist\")\n",
    "        return chunks\n",
    "    \n",
    "    json_files = [f for f in commentaries_dir.glob(\"*.json\") if f.name not in skip_files]\n",
    "    print(f\"üìÇ Found {len(json_files)} remaining commentary JSON files\")\n",
    "    \n",
    "    for file in tqdm(json_files, desc=\"Loading remaining commentaries\"):\n",
    "        try:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                commentary_data = json.load(f)\n",
    "                for entry in commentary_data:\n",
    "                    chunks.append({\n",
    "                        \"text\": entry[\"text\"],\n",
    "                        \"metadata\": entry[\"metadata\"]\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {file.name}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(chunks)} remaining commentary chunks\")\n",
    "    return chunks\n",
    "\n",
    "# üîÅ Run this to upload the rest\n",
    "remaining_commentary_chunks = load_remaining_commentaries(\n",
    "    \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/batch2/commentaries/\"\n",
    ")\n",
    "\n",
    "embed_and_upload(remaining_commentary_chunks, \"commentaries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40167d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_interviews(interview_path):\n",
    "    \"\"\"Load interview chunks from JSON file with flat structure.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    path = Path(interview_path)\n",
    "    if not path.exists():\n",
    "        print(f\"‚ö†Ô∏è Interview file {path} does not exist\")\n",
    "        return chunks\n",
    "    \n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            interview_data = json.load(f)\n",
    "            for entry in interview_data:\n",
    "                metadata = entry.copy()\n",
    "                text = metadata.pop(\"text\")  # remove text from metadata\n",
    "                chunks.append({\n",
    "                    \"text\": text,\n",
    "                    \"metadata\": metadata\n",
    "                })\n",
    "        print(f\"‚úÖ Loaded {len(chunks)} interview chunks\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading interview chunks: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83413591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 770 interview chunks\n",
      "üì¶ Processing 770 chunks for collection interviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/s2pyk63x5958wcxdtr6xlpwc0000gn/T/ipykernel_36363/1754979787.py:6: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created collection: interviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 1/8 to interviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 2/8 to interviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 3/8 to interviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 4/8 to interviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 5/8 to interviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 6/8 to interviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 7/8 to interviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 8/8 to interviews\n"
     ]
    }
   ],
   "source": [
    "interview_path = \"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/PSAI/chunks/batch2/interview.json\"\n",
    "\n",
    "interview_chunks = load_interviews(interview_path)\n",
    "embed_and_upload(interview_chunks, \"interviews\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
