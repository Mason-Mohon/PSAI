{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf05d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 69 chunks to D:\\Technical_projects\\PSAI\\chunks\\books\\How_the_Republican_Party_Became_Pro-Life.json\n",
      "Saved 217 chunks to D:\\Technical_projects\\PSAI\\chunks\\books\\Who_Killed_the_American_Family.json\n",
      "Saved 407 chunks to D:\\Technical_projects\\PSAI\\chunks\\books\\The_Supremacists.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import docx\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "from textwrap import wrap\n",
    "\n",
    "def extract_text_docx(filepath):\n",
    "    doc = docx.Document(filepath)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "def extract_text_pdf(filepath):\n",
    "    doc = fitz.open(filepath)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "def chunk_text(text, max_length=1000):\n",
    "    paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "    chunks, current_chunk = [], \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) + 1 <= max_length:\n",
    "            current_chunk += para + \"\\n\"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = para + \"\\n\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_book(filepath, book_title, year, output_folder):\n",
    "    ext = filepath.suffix.lower()\n",
    "    if ext == \".docx\":\n",
    "        text = extract_text_docx(filepath)\n",
    "    elif ext == \".pdf\":\n",
    "        text = extract_text_pdf(filepath)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath.name}\")\n",
    "\n",
    "    chunks = chunk_text(text)\n",
    "    json_chunks = [\n",
    "        {\n",
    "            \"author\": \"Phyllis Schlafly\",\n",
    "            \"book_title\": book_title,\n",
    "            \"publication_year\": year,\n",
    "            \"text\": chunk\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "    output_path = output_folder / f\"{book_title.replace(' ', '_')}.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Saved {len(json_chunks)} chunks to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "book_info = [\n",
    "    (\"LifeBook2015 - MANUSCRIPT.docx\", \"How the Republican Party Became Pro-Life\", 2015),\n",
    "    (\"Who Killed the American Family - EDITED.docx\", \"Who Killed the American Family\", 2014),\n",
    "    (\"The_Supremacists_by_Phyllis_Schlafly.pdf\", \"The Supremacists\", 2004)\n",
    "]\n",
    "\n",
    "input_folder = Path(r\"D:\\Technical_projects\\PSAI\\raw_data\\books\")\n",
    "output_folder = Path(r\"D:\\Technical_projects\\PSAI\\chunks\\books\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename, title, year in book_info:\n",
    "    process_book(input_folder / filename, title, year, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
